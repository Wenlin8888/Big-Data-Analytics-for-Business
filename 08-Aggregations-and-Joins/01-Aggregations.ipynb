{"cells": [{"cell_type": "markdown", "metadata": {}, "source": "# Aggregations\n\nSpark allows us to create several different grouping types for aggregation. This notebook will discuss some of these grouping and aggregation techniques.\n\nLet's first load some retail data. We have over 300 csv files each representing daily transactions in a retail store:"}, {"cell_type": "code", "execution_count": 1, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "gs://qst843/Big-Data-Analytics-for-Business/data/\n"}], "source": "# the following line gets the bucket name attached to our cluster\nbucket = spark._jsc.hadoopConfiguration().get(\"fs.gs.system.bucket\")\n\n# specifying the path to our bucket where the data is located (no need to edit this path anymore)\ndata = \"gs://\" + bucket + \"/Big-Data-Analytics-for-Business/data/\"\nprint(data)"}, {"cell_type": "markdown", "metadata": {}, "source": "## Data import"}, {"cell_type": "code", "execution_count": 2, "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "df = spark.read.format(\"csv\")\\\n  .option(\"header\", \"true\")\\\n  .option(\"inferSchema\", \"true\")\\\n  .load(data + \"retail-data/by-day/*.csv\")"}, {"cell_type": "markdown", "metadata": {}, "source": "Here\u2019s a sample of the data:"}, {"cell_type": "code", "execution_count": 3, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+---------+---------+-------------------------------+--------+-------------------+---------+----------+--------------+\n|InvoiceNo|StockCode|Description                    |Quantity|InvoiceDate        |UnitPrice|CustomerID|Country       |\n+---------+---------+-------------------------------+--------+-------------------+---------+----------+--------------+\n|580538   |23084    |RABBIT NIGHT LIGHT             |48      |2011-12-05 08:38:00|1.79     |14075.0   |United Kingdom|\n|580538   |23077    |DOUGHNUT LIP GLOSS             |20      |2011-12-05 08:38:00|1.25     |14075.0   |United Kingdom|\n|580538   |22906    |12 MESSAGE CARDS WITH ENVELOPES|24      |2011-12-05 08:38:00|1.65     |14075.0   |United Kingdom|\n|580538   |21914    |BLUE HARMONICA IN BOX          |24      |2011-12-05 08:38:00|1.25     |14075.0   |United Kingdom|\n|580538   |22467    |GUMBALL COAT RACK              |6       |2011-12-05 08:38:00|2.55     |14075.0   |United Kingdom|\n+---------+---------+-------------------------------+--------+-------------------+---------+----------+--------------+\nonly showing top 5 rows\n\nroot\n |-- InvoiceNo: string (nullable = true)\n |-- StockCode: string (nullable = true)\n |-- Description: string (nullable = true)\n |-- Quantity: integer (nullable = true)\n |-- InvoiceDate: timestamp (nullable = true)\n |-- UnitPrice: double (nullable = true)\n |-- CustomerID: double (nullable = true)\n |-- Country: string (nullable = true)\n\n"}], "source": "df.show(5, False)\n\ndf.printSchema()"}, {"cell_type": "markdown", "metadata": {}, "source": "`InvoiceDate` is being recognized as string. Let's replace it with a date format. We won't need the timestamp, so it's ok to lose it.\n\nWe will also cast `CustomerID` as string and Quantity as long:"}, {"cell_type": "code", "execution_count": 4, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+---------+---------+--------------------+--------+-----------+---------+----------+--------------+\n|InvoiceNo|StockCode|         Description|Quantity|InvoiceDate|UnitPrice|CustomerID|       Country|\n+---------+---------+--------------------+--------+-----------+---------+----------+--------------+\n|   580538|    23084|  RABBIT NIGHT LIGHT|      48| 2011-12-05|     1.79|   14075.0|United Kingdom|\n|   580538|    23077| DOUGHNUT LIP GLOSS |      20| 2011-12-05|     1.25|   14075.0|United Kingdom|\n|   580538|    22906|12 MESSAGE CARDS ...|      24| 2011-12-05|     1.65|   14075.0|United Kingdom|\n|   580538|    21914|BLUE HARMONICA IN...|      24| 2011-12-05|     1.25|   14075.0|United Kingdom|\n|   580538|    22467|   GUMBALL COAT RACK|       6| 2011-12-05|     2.55|   14075.0|United Kingdom|\n+---------+---------+--------------------+--------+-----------+---------+----------+--------------+\nonly showing top 5 rows\n\nroot\n |-- InvoiceNo: string (nullable = true)\n |-- StockCode: string (nullable = true)\n |-- Description: string (nullable = true)\n |-- Quantity: long (nullable = true)\n |-- InvoiceDate: date (nullable = true)\n |-- UnitPrice: double (nullable = true)\n |-- CustomerID: string (nullable = true)\n |-- Country: string (nullable = true)\n\n"}], "source": "from pyspark.sql.functions import col, to_date\n\ndf = df.withColumn(\"InvoiceDate\", to_date(col(\"InvoiceDate\"), \"yyyy-MM-dd HH:mm:ss\"))\n\ndf = df.withColumn(\"CustomerID\", col(\"CustomerID\").cast(\"string\"))\ndf = df.withColumn(\"Quantity\", col(\"Quantity\").cast(\"long\"))\n\ndf.createOrReplaceTempView(\"dfTable\")\n\ndf.show(5)\ndf.printSchema()"}, {"cell_type": "markdown", "metadata": {}, "source": "## Caching\n\nCaching allows the DataFrame to be loaded and persist in the memory. If we don't use this option, every time we execute an action our DataFrame gets loaded from our Cloud Storage, which is not ideal and will add to our execution time:\n\n**Note:** Caching is a lazy transformation. It will happen the first time you execute an action against the DataFrame, not when you cache that DataFrame."}, {"cell_type": "code", "execution_count": 5, "metadata": {}, "outputs": [{"data": {"text/plain": "DataFrame[InvoiceNo: string, StockCode: string, Description: string, Quantity: bigint, InvoiceDate: date, UnitPrice: double, CustomerID: string, Country: string]"}, "execution_count": 5, "metadata": {}, "output_type": "execute_result"}], "source": "df.cache()"}, {"cell_type": "markdown", "metadata": {}, "source": "Basic aggregations apply to an entire DataFrame. The simplest example is the `count` method:"}, {"cell_type": "code", "execution_count": 6, "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"data": {"text/plain": "541909"}, "execution_count": 6, "metadata": {}, "output_type": "execute_result"}], "source": "df.count()"}, {"cell_type": "markdown", "metadata": {}, "source": "It returns the total number of rows in this DataFrame. `count` when used in this context is actually an action, so it returns the output immediately. We will also encounter cases that `count` will be acting as a lazy transformation.\n\nNow that we have performed an action on `df` it should be cached into the memory. Go ahead an execute the previous command again to see the performance gain:"}, {"cell_type": "code", "execution_count": 7, "metadata": {}, "outputs": [{"data": {"text/plain": "541909"}, "execution_count": 7, "metadata": {}, "output_type": "execute_result"}], "source": "df.count()"}, {"cell_type": "markdown", "metadata": {}, "source": "Once done with the DataFrame we can free up the memory by removing the cache. This can be done by `unpersist`:\n```python\ndf.unpersist()\n```"}, {"cell_type": "markdown", "metadata": {}, "source": "## Aggregation Functions\n\nAll aggregations are available as functions, in addition to the special cases that can appear on DataFrames or via `.stat`. You can find most aggregation functions in the `pyspark.sql.functions` package.\n\n**Note:** There are some gaps between the available SQL functions and the functions that we can import in Scala and Python. This changes every release, so it\u2019s impossible to include a definitive list. This section covers the most common functions.\n\n### `count`\nThe first function worth going over is `count`, except in this example it will perform as a transformation instead of an action. In this case, we can do one of two things: specify a specific column to count, or all the columns by using `count(*)` or `count(1)` to represent that we want to count every row as the literal one, as shown in this example:"}, {"cell_type": "code", "execution_count": 8, "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "[Stage 13:============================>                            (5 + 2) / 10]\r"}, {"name": "stdout", "output_type": "stream", "text": "+----------------+\n|count(StockCode)|\n+----------------+\n|          541909|\n+----------------+\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "from pyspark.sql.functions import count\n\ndf.select(count(\"StockCode\")).show()"}, {"cell_type": "markdown", "metadata": {}, "source": "In SQL\n```sql\nSELECT COUNT(*) FROM dfTable\n```\n\n**Warning**\n\nwhen performing a `count(*)`, Spark will count null values (including rows containing all nulls). However, when counting an individual column, Spark will not count the null values. For instance if we repeate this for \"CustomerID\" column we will get a different value due to the null values:"}, {"cell_type": "code", "execution_count": 9, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+-----------------+\n|count(CustomerID)|\n+-----------------+\n|           406829|\n+-----------------+\n\n"}], "source": "df.select(count(\"CustomerID\")).show()"}, {"cell_type": "markdown", "metadata": {}, "source": "### `countDistinct`\n\nSometimes, the total number is not relevant; rather, it\u2019s the number of unique groups that you want. To get this number, you can use the `countDistinct` function. This is a bit more relevant for individual columns:"}, {"cell_type": "code", "execution_count": 10, "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "[Stage 19:===================================================>     (9 + 1) / 10]\r"}, {"name": "stdout", "output_type": "stream", "text": "+-------------------------+\n|count(DISTINCT StockCode)|\n+-------------------------+\n|                     4070|\n+-------------------------+\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "from pyspark.sql.functions import countDistinct\n\ndf.select(countDistinct(\"StockCode\")).show()"}, {"cell_type": "markdown", "metadata": {}, "source": "In SQL\n```sql\nSELECT COUNT(DISTINCT *) FROM DFTABLE\n```\n\n`approx_count_distinct`: This function can be used to estimate the count distinct. It will give us a lot of performace gain.\n\n### `min` and `max`\n\nTo extract the minimum and maximum values from a DataFrame, use the min and max functions:"}, {"cell_type": "code", "execution_count": 11, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+-------------+-------------+\n|min(Quantity)|max(Quantity)|\n+-------------+-------------+\n|       -80995|        80995|\n+-------------+-------------+\n\n"}], "source": "from pyspark.sql.functions import min, max\n\ndf.select(min(\"Quantity\"), max(\"Quantity\")).show()"}, {"cell_type": "markdown", "metadata": {}, "source": "### `sum`\n\nAnother simple task is to add all the values in a row using the sum function:"}, {"cell_type": "code", "execution_count": 12, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+-------------+\n|sum(Quantity)|\n+-------------+\n|      5176450|\n+-------------+\n\n"}], "source": "from pyspark.sql.functions import sum\n\ndf.select(sum(\"Quantity\")).show()"}, {"cell_type": "markdown", "metadata": {}, "source": "### `avg`\n\n`avg` or `mean` functions:"}, {"cell_type": "code", "execution_count": 13, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+--------------------------------------+----------------+----------------+\n|(total_purchases / total_transactions)|   avg_purchases|  mean_purchases|\n+--------------------------------------+----------------+----------------+\n|                      9.55224954743324|9.55224954743324|9.55224954743324|\n+--------------------------------------+----------------+----------------+\n\n"}], "source": "from pyspark.sql.functions import sum, count, avg, expr\n\ndf.select(\n    count(\"Quantity\").alias(\"total_transactions\"),\n    sum(\"Quantity\").alias(\"total_purchases\"),\n    avg(\"Quantity\").alias(\"avg_purchases\"),\n    expr(\"mean(Quantity)\").alias(\"mean_purchases\"))\\\n  .selectExpr(\n    \"total_purchases/total_transactions\",\n    \"avg_purchases\",\n    \"mean_purchases\").show()"}, {"cell_type": "markdown", "metadata": {}, "source": "### Variance and Standard Deviation\n\nCalculating the mean naturally brings up questions about the variance and standard deviation. These are both measures of the spread of the data around the mean. The variance is the average of the squared differences from the mean, and the standard deviation is the square root of the variance. You can calculate these in Spark by using their respective functions:"}, {"cell_type": "code", "execution_count": 14, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+------------------+-----------------+\n|var_samp(Quantity)| stddev(Quantity)|\n+------------------+-----------------+\n| 47559.39140929846|218.0811578502335|\n+------------------+-----------------+\n\n"}], "source": "from pyspark.sql.functions import variance, stddev\n\ndf.select(variance(\"Quantity\"), stddev(\"Quantity\")).show()"}, {"cell_type": "markdown", "metadata": {}, "source": "Something to note is that Spark has both the formula for the sample standard deviation as well as the formula for the population standard deviation. These are fundamentally different statistical formulae, and we need to differentiate between them. By default, Spark performs the formula for the sample standard deviation or variance if you use the `variance` or `stddev` functions.\n\nYou can also specify these explicitly or refer to the population standard deviation or variance:"}, {"cell_type": "code", "execution_count": 15, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+------------------+------------------+--------------------+---------------------+\n| var_pop(Quantity)|var_samp(Quantity)|stddev_pop(Quantity)|stddev_samp(Quantity)|\n+------------------+------------------+--------------------+---------------------+\n|47559.303646608765|47559.391409298456|   218.0809566344773|    218.0811578502335|\n+------------------+------------------+--------------------+---------------------+\n\n"}], "source": "from pyspark.sql.functions import var_pop, stddev_pop\nfrom pyspark.sql.functions import var_samp, stddev_samp\n\ndf.select(var_pop(\"Quantity\"), var_samp(\"Quantity\"),\n  stddev_pop(\"Quantity\"), stddev_samp(\"Quantity\")).show()"}, {"cell_type": "markdown", "metadata": {}, "source": "In SQL\n```sql\nSELECT var_pop(Quantity), var_samp(Quantity),\n  stddev_pop(Quantity), stddev_samp(Quantity)\nFROM dfTable\n```\n\n### Covariance and Correlation\n\nWe discussed single column aggregations, but some functions compare the interactions of the values in two difference columns together. Two of these functions are `covar_samp` and `corr`, for covariance and correlation, respectively. Correlation measures the Pearson correlation coefficient, which is scaled between \u20131 and +1. The covariance is scaled according to the inputs in the data. There also exists a covariance function for the population, covar_pop."}, {"cell_type": "code", "execution_count": 16, "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "[Stage 40:==================================>                      (6 + 2) / 10]\r"}, {"name": "stdout", "output_type": "stream", "text": "+-------------------------+-------------------------------+\n|corr(UnitPrice, Quantity)|covar_samp(UnitPrice, Quantity)|\n+-------------------------+-------------------------------+\n|     -0.00123492454487...|             -26.05876125793664|\n+-------------------------+-------------------------------+\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "from pyspark.sql.functions import corr, covar_samp\n\ndf.select(corr(\"UnitPrice\", \"Quantity\"), covar_samp(\"UnitPrice\", \"Quantity\")).show()"}, {"cell_type": "markdown", "metadata": {}, "source": "## Grouping\n\nThus far, we have performed only DataFrame-level aggregations. A more common task is to perform calculations based on groups in the data. This is typically done on categorical data for which we group our data on one column and perform some calculations on the other columns that end up in that group.\n\nThe best way to explain this is to begin performing some groupings. The first will be a count, just as we did before. We will group by each unique invoice number and get the count of items on that invoice. Note that this returns another DataFrame and is lazily performed.\n\nWe do this grouping in two phases. First we specify the column(s) on which we would like to group, and then we specify the aggregation(s). The first step returns a `RelationalGroupedDataset`, and the second step returns a `DataFrame`.\n\nAs mentioned, we can specify any number of columns on which we want to group:"}, {"cell_type": "code", "execution_count": 17, "metadata": {}, "outputs": [{"data": {"text/plain": "GroupedData[grouping expressions: [InvoiceNo, CustomerId], value: [InvoiceNo: string, StockCode: string ... 6 more fields], type: GroupBy]"}, "execution_count": 17, "metadata": {}, "output_type": "execute_result"}], "source": "df.groupBy(\"InvoiceNo\", \"CustomerId\")"}, {"cell_type": "code", "execution_count": 18, "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "[Stage 43:=======================================>                 (7 + 3) / 10]\r"}, {"name": "stdout", "output_type": "stream", "text": "+---------+----------+-----+\n|InvoiceNo|CustomerId|count|\n+---------+----------+-----+\n|   576346|   13835.0|    6|\n|   570421|   13980.0|   11|\n|   574478|   17203.0|   34|\n|   580739|      NULL|    2|\n|   567695|      NULL|    1|\n+---------+----------+-----+\nonly showing top 5 rows\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "df.groupBy(\"InvoiceNo\", \"CustomerId\").count().show(5)"}, {"cell_type": "markdown", "metadata": {}, "source": "In SQL\n```sql\nSELECT InvoiceNo, CustomerId, count(*) FROM dfTable GROUP BY InvoiceNo, CustomerId\n```\n\n## Grouping with Expressions\n\nAs we saw earlier, counting is a bit of a special case because it exists as a method. For this, usually we prefer to use the `count` function. Rather than passing that function as an expression into a `select` statement, we specify it as within `agg`. This makes it possible for you to pass-in arbitrary expressions that just need to have some aggregation specified. You can even do things like `alias` a column after transforming it for later use in your data flow:"}, {"cell_type": "code", "execution_count": 19, "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "[Stage 46:===================================================>     (9 + 1) / 10]\r"}, {"name": "stdout", "output_type": "stream", "text": "+---------+----+-----+\n|InvoiceNo|quan|quan2|\n+---------+----+-----+\n|   578057|  28|   28|\n|  C578132|   1|    1|\n|   576112|  20|   20|\n|   580739|   2|    2|\n|   567695|   1|    1|\n+---------+----+-----+\nonly showing top 5 rows\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "from pyspark.sql.functions import count, expr\n\ndf.groupBy(\"InvoiceNo\")\\\n  .agg(count(\"Quantity\").alias(\"quan\"),\n       expr(\"count(Quantity) as quan2\")\n      ).show(5)"}, {"cell_type": "code", "execution_count": 20, "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "[Stage 49:============================>                            (5 + 4) / 10]\r"}, {"name": "stdout", "output_type": "stream", "text": "+---------+-----------------+--------------------+\n|InvoiceNo|    avg(Quantity)|stddev_pop(Quantity)|\n+---------+-----------------+--------------------+\n|   578057|4.607142857142857|   8.755974636597271|\n|  C578132|             -1.0|                 0.0|\n|   576112|             10.9|  7.4959989327640635|\n|   580739|              2.5|                 0.5|\n|   567695|            -42.0|                 0.0|\n+---------+-----------------+--------------------+\nonly showing top 5 rows\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "df.groupBy(\"InvoiceNo\").agg(expr(\"avg(Quantity)\"),expr(\"stddev_pop(Quantity)\")).show(5)"}, {"cell_type": "markdown", "metadata": {}, "source": "## Window Functions\n\nWindow functions operate on a set of rows and return a single value for each row from the underlying query. The term window describes the set of rows on which the function operates. A window function uses values from the rows in a window to calculate the returned values.\n\nThe following SQL query adds a new column (`overall_dataset_Q`) that includes the overall quantity for the entire dataset. This value will be the same for all the rows:"}, {"cell_type": "code", "execution_count": 21, "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "25/10/20 18:19:17 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/10/20 18:19:17 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/10/20 18:19:17 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/10/20 18:19:17 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/10/20 18:19:17 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/10/20 18:19:18 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/10/20 18:19:18 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n[Stage 54:>                                                         (0 + 1) / 1]\r"}, {"name": "stdout", "output_type": "stream", "text": "+----------+-----------+--------+-----------------+\n|CustomerId|InvoiceDate|Quantity|overall_dataset_Q|\n+----------+-----------+--------+-----------------+\n|   12346.0| 2011-01-18|   74215|          4906888|\n|   12346.0| 2011-01-18|  -74215|          4906888|\n|   12347.0| 2011-12-07|      12|          4906888|\n|   12347.0| 2011-12-07|      24|          4906888|\n|   12347.0| 2011-12-07|       6|          4906888|\n+----------+-----------+--------+-----------------+\nonly showing top 5 rows\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "spark.sql(\"\"\"\nSELECT CustomerId, InvoiceDate, Quantity,\n\n  sum(Quantity) OVER () as overall_dataset_Q\n  \nFROM dfTable WHERE CustomerId IS NOT NULL \nORDER BY CustomerId, InvoiceDate DESC\n\"\"\").show(5)"}, {"cell_type": "markdown", "metadata": {}, "source": "`OVER()` clause indicates that we want to use a window over the `sum()` expression. \n\nThe `OVER()` clause has the following capabilities:\n\n* Defines window partitions to form groups of rows. (`PARTITION BY` clause)\n* Orders rows within a partition. (`ORDER BY` clause)\n\nIn the above query since we haven't defined a `PARTITION` within `OVER()` the aggregating function applies to the entire dataset.\n\nWe can do a lot more useful calculations with the window functions. For instance, we can add a total quantity per customer and a total quantity per customer by date, specifying the appropriate `PARTITION`s:"}, {"cell_type": "code", "execution_count": 22, "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "25/10/20 18:19:19 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/10/20 18:19:19 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/10/20 18:19:19 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/10/20 18:19:19 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/10/20 18:19:20 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/10/20 18:19:20 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n[Stage 57:>                                                         (0 + 1) / 1]\r"}, {"name": "stdout", "output_type": "stream", "text": "+----------+-----------+--------+-----------------+-------+---------------+\n|CustomerId|InvoiceDate|Quantity|overall_dataset_Q|total_Q|total_Q_by_date|\n+----------+-----------+--------+-----------------+-------+---------------+\n|   12346.0| 2011-01-18|   74215|          4906888|      0|              0|\n|   12346.0| 2011-01-18|  -74215|          4906888|      0|              0|\n|   12347.0| 2011-12-07|      12|          4906888|   2458|            192|\n|   12347.0| 2011-12-07|       6|          4906888|   2458|            192|\n|   12347.0| 2011-12-07|      24|          4906888|   2458|            192|\n|   12347.0| 2011-12-07|      24|          4906888|   2458|            192|\n|   12347.0| 2011-12-07|      24|          4906888|   2458|            192|\n|   12347.0| 2011-12-07|      24|          4906888|   2458|            192|\n|   12347.0| 2011-12-07|      10|          4906888|   2458|            192|\n|   12347.0| 2011-12-07|      12|          4906888|   2458|            192|\n|   12347.0| 2011-12-07|      16|          4906888|   2458|            192|\n|   12347.0| 2011-12-07|      20|          4906888|   2458|            192|\n|   12347.0| 2011-12-07|      20|          4906888|   2458|            192|\n|   12347.0| 2011-10-31|       4|          4906888|   2458|            676|\n|   12347.0| 2011-10-31|      12|          4906888|   2458|            676|\n|   12347.0| 2011-10-31|       8|          4906888|   2458|            676|\n|   12347.0| 2011-10-31|      12|          4906888|   2458|            676|\n|   12347.0| 2011-10-31|      12|          4906888|   2458|            676|\n|   12347.0| 2011-10-31|      10|          4906888|   2458|            676|\n|   12347.0| 2011-10-31|      20|          4906888|   2458|            676|\n+----------+-----------+--------+-----------------+-------+---------------+\nonly showing top 20 rows\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "spark.sql(\"\"\"\nSELECT CustomerId, InvoiceDate, Quantity,\n\n  sum(Quantity) OVER () as overall_dataset_Q,\n  \n  sum(Quantity) OVER (PARTITION BY CustomerId) as total_Q,\n  \n  sum(Quantity) OVER (PARTITION BY CustomerId, InvoiceDate) as total_Q_by_date\n  \nFROM dfTable WHERE CustomerId IS NOT NULL \nORDER BY CustomerId, InvoiceDate DESC\n\"\"\").show()"}, {"cell_type": "markdown", "metadata": {}, "source": "Using `ORDER BY` clause we can sort the rows within the `PARTITION`s and then use the `rank()` function to give them a ranking:"}, {"cell_type": "code", "execution_count": 23, "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "25/10/20 18:19:22 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/10/20 18:19:22 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/10/20 18:19:22 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/10/20 18:19:22 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/10/20 18:19:22 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/10/20 18:19:22 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n[Stage 60:>                                                         (0 + 1) / 1]\r"}, {"name": "stdout", "output_type": "stream", "text": "+----------+-----------+--------+-----------------+-------+---------------+----+\n|CustomerId|InvoiceDate|Quantity|overall_dataset_Q|total_Q|total_Q_by_date|rank|\n+----------+-----------+--------+-----------------+-------+---------------+----+\n|   12346.0| 2011-01-18|   74215|          4906888|      0|              0|   1|\n|   12346.0| 2011-01-18|  -74215|          4906888|      0|              0|   2|\n|   12347.0| 2011-12-07|      24|          4906888|   2458|            192|   1|\n|   12347.0| 2011-12-07|      24|          4906888|   2458|            192|   1|\n|   12347.0| 2011-12-07|      24|          4906888|   2458|            192|   1|\n|   12347.0| 2011-12-07|      24|          4906888|   2458|            192|   1|\n|   12347.0| 2011-12-07|      20|          4906888|   2458|            192|   5|\n|   12347.0| 2011-12-07|      20|          4906888|   2458|            192|   5|\n|   12347.0| 2011-12-07|      16|          4906888|   2458|            192|   7|\n|   12347.0| 2011-12-07|      12|          4906888|   2458|            192|   8|\n|   12347.0| 2011-12-07|      12|          4906888|   2458|            192|   8|\n|   12347.0| 2011-12-07|      10|          4906888|   2458|            192|  10|\n|   12347.0| 2011-12-07|       6|          4906888|   2458|            192|  11|\n|   12347.0| 2011-10-31|      48|          4906888|   2458|            676|   1|\n|   12347.0| 2011-10-31|      36|          4906888|   2458|            676|   2|\n|   12347.0| 2011-10-31|      36|          4906888|   2458|            676|   2|\n|   12347.0| 2011-10-31|      24|          4906888|   2458|            676|   4|\n|   12347.0| 2011-10-31|      24|          4906888|   2458|            676|   4|\n|   12347.0| 2011-10-31|      24|          4906888|   2458|            676|   4|\n|   12347.0| 2011-10-31|      24|          4906888|   2458|            676|   4|\n+----------+-----------+--------+-----------------+-------+---------------+----+\nonly showing top 20 rows\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "spark.sql(\"\"\"\nSELECT CustomerId, InvoiceDate, Quantity,\n\n  sum(Quantity) OVER () as overall_dataset_Q,\n  \n  sum(Quantity) OVER (PARTITION BY CustomerId) as total_Q,\n  \n  sum(Quantity) OVER (PARTITION BY CustomerId, InvoiceDate) as total_Q_by_date,\n  \n  RANK() OVER (PARTITION BY CustomerId, InvoiceDate \n               ORDER BY Quantity DESC) as rank\n  \nFROM dfTable WHERE CustomerId IS NOT NULL \nORDER BY CustomerId, InvoiceDate DESC, rank\n\"\"\").show()"}, {"cell_type": "markdown", "metadata": {}, "source": "Please notice the behavior of `rank()` when it comes across tied values.\n\n`dense_rank()` returns the rank of rows within a window partition, without any gaps.\n\n`row_number()` returns a sequential number starting at 1 within a window partition. Please note that the ties will be numbered at random. This could have downstream consequences!\n\nIn the example below we have added both `dense_rank()` and `row_number()` to the previous query:"}, {"cell_type": "code", "execution_count": 24, "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "25/10/20 18:19:26 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/10/20 18:19:26 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/10/20 18:19:26 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/10/20 18:19:26 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/10/20 18:19:27 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/10/20 18:19:27 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n[Stage 63:>                                                         (0 + 1) / 1]\r"}, {"name": "stdout", "output_type": "stream", "text": "+----------+-----------+--------+-----------------+-------+-----+---------------+----+------+----------+\n|CustomerId|InvoiceDate|Quantity|overall_dataset_Q|total_Q|max_Q|total_Q_by_date|rank|d_rank|row_number|\n+----------+-----------+--------+-----------------+-------+-----+---------------+----+------+----------+\n|   12346.0| 2011-01-18|   74215|          4906888|      0|74215|              0|   1|     1|         1|\n|   12346.0| 2011-01-18|  -74215|          4906888|      0|74215|              0|   2|     2|         2|\n|   12347.0| 2011-12-07|      24|          4906888|   2458|  240|            192|   1|     1|         1|\n|   12347.0| 2011-12-07|      24|          4906888|   2458|  240|            192|   1|     1|         2|\n|   12347.0| 2011-12-07|      24|          4906888|   2458|  240|            192|   1|     1|         3|\n|   12347.0| 2011-12-07|      24|          4906888|   2458|  240|            192|   1|     1|         4|\n|   12347.0| 2011-12-07|      20|          4906888|   2458|  240|            192|   5|     2|         5|\n|   12347.0| 2011-12-07|      20|          4906888|   2458|  240|            192|   5|     2|         6|\n|   12347.0| 2011-12-07|      16|          4906888|   2458|  240|            192|   7|     3|         7|\n|   12347.0| 2011-12-07|      12|          4906888|   2458|  240|            192|   8|     4|         8|\n|   12347.0| 2011-12-07|      12|          4906888|   2458|  240|            192|   8|     4|         9|\n|   12347.0| 2011-12-07|      10|          4906888|   2458|  240|            192|  10|     5|        10|\n|   12347.0| 2011-12-07|       6|          4906888|   2458|  240|            192|  11|     6|        11|\n|   12347.0| 2011-10-31|      48|          4906888|   2458|  240|            676|   1|     1|         1|\n|   12347.0| 2011-10-31|      36|          4906888|   2458|  240|            676|   2|     2|         2|\n|   12347.0| 2011-10-31|      36|          4906888|   2458|  240|            676|   2|     2|         3|\n|   12347.0| 2011-10-31|      24|          4906888|   2458|  240|            676|   4|     3|         4|\n|   12347.0| 2011-10-31|      24|          4906888|   2458|  240|            676|   4|     3|         5|\n|   12347.0| 2011-10-31|      24|          4906888|   2458|  240|            676|   4|     3|         6|\n|   12347.0| 2011-10-31|      24|          4906888|   2458|  240|            676|   4|     3|         7|\n+----------+-----------+--------+-----------------+-------+-----+---------------+----+------+----------+\nonly showing top 20 rows\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "spark.sql(\"\"\"\nSELECT CustomerId, InvoiceDate, Quantity,\n\n  sum(Quantity) OVER () as overall_dataset_Q,\n  \n  sum(Quantity) OVER (PARTITION BY CustomerId) as total_Q,\n  \n  max(Quantity) OVER (PARTITION BY CustomerId) as max_Q,\n\n  sum(Quantity) OVER (PARTITION BY CustomerId, InvoiceDate) as total_Q_by_date,\n    \n  RANK() OVER (PARTITION BY CustomerId, InvoiceDate \n               ORDER BY Quantity DESC) as rank,\n  \n  DENSE_RANK() OVER (PARTITION BY CustomerId, InvoiceDate \n                     ORDER BY Quantity DESC) as d_rank,\n  \n  ROW_NUMBER() OVER (PARTITION BY CustomerId, InvoiceDate \n                     ORDER BY Quantity DESC) as row_number\n  \nFROM dfTable WHERE CustomerId IS NOT NULL \nORDER BY CustomerId, InvoiceDate DESC, row_number\n\"\"\").show()"}, {"cell_type": "markdown", "metadata": {}, "source": "A few points regarding ranking functions:\n\n* `ROW_NUMBER()`, `RANK()`, and other ranking functions must always be windowed and therefore cannot appear without a corresponding OVER clause.\n\n* Give consideration to how ties should be handled with ranking functions. If you need contiguous ranking, you should use `DENSE_RANK()` instead.\n\n* The `ORDER BY` predicate is mandatory for this class of functions because it influences how the results will be sequenced or ranked."}, {"cell_type": "markdown", "metadata": {}, "source": "### PySpark Syntax\n\nNow that we are familiar with the SQL syntax of window functions let's have a look at its PySpark equivalent.\n\nThe first step to a window function is to create a window specification. \n\nNote that the `partition by` is unrelated to the partitioning scheme concept that we have covered thus far. It\u2019s just a similar concept that describes how we will be breaking up our group. The ordering determines the ordering within a given partition. \n\nIn the following example, we will reproduce our last SQL query:"}, {"cell_type": "code", "execution_count": 25, "metadata": {}, "outputs": [], "source": "from pyspark.sql.window import Window\nfrom pyspark.sql.functions import desc\n\nwindowSpec0 = Window.partitionBy()\nwindowSpec1 = Window.partitionBy(\"CustomerId\")\nwindowSpec2 = Window.partitionBy(\"CustomerId\", \"InvoiceDate\")\nwindowSpec3 = Window.partitionBy(\"CustomerId\", \"InvoiceDate\").orderBy(desc(\"Quantity\"))"}, {"cell_type": "markdown", "metadata": {}, "source": "Now we want to use an aggregation function to learn more about each specific customer. An example might be establishing the maximum purchase quantity on each day. We indicate the window specification that defines to which frames of data this function will apply:"}, {"cell_type": "code", "execution_count": 26, "metadata": {}, "outputs": [], "source": "from pyspark.sql.functions import sum, max, rank, dense_rank, row_number\n\noverall_dataset_Q = sum(col(\"Quantity\")).over(windowSpec0)\ntotal_Q = sum(col(\"Quantity\")).over(windowSpec1)\nmax_Q = max(col(\"Quantity\")).over(windowSpec1)\ntotal_Q_by_date = sum(col(\"Quantity\")).over(windowSpec2)\nrank = rank().over(windowSpec3)\nd_rank = dense_rank().over(windowSpec3)\nrow_number = row_number().over(windowSpec3)"}, {"cell_type": "markdown", "metadata": {}, "source": "This returns columns that we can use in `select` statements. Now we can perform a select to view the calculated window values:"}, {"cell_type": "code", "execution_count": 27, "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "25/10/20 18:19:29 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/10/20 18:19:29 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/10/20 18:19:29 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/10/20 18:19:29 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/10/20 18:19:29 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/10/20 18:19:29 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n[Stage 66:>                                                         (0 + 1) / 1]\r"}, {"name": "stdout", "output_type": "stream", "text": "+----------+-----------+--------+-----------------+-------+-----+---------------+----+------+----------+\n|CustomerId|InvoiceDate|Quantity|overall_dataset_Q|total_Q|max_Q|total_Q_by_date|rank|d_rank|row_number|\n+----------+-----------+--------+-----------------+-------+-----+---------------+----+------+----------+\n|   12346.0| 2011-01-18|   74215|          4906888|      0|74215|              0|   1|     1|         1|\n|   12346.0| 2011-01-18|  -74215|          4906888|      0|74215|              0|   2|     2|         2|\n|   12347.0| 2011-12-07|      24|          4906888|   2458|  240|            192|   1|     1|         1|\n|   12347.0| 2011-12-07|      24|          4906888|   2458|  240|            192|   1|     1|         2|\n|   12347.0| 2011-12-07|      24|          4906888|   2458|  240|            192|   1|     1|         3|\n|   12347.0| 2011-12-07|      24|          4906888|   2458|  240|            192|   1|     1|         4|\n|   12347.0| 2011-12-07|      20|          4906888|   2458|  240|            192|   5|     2|         5|\n|   12347.0| 2011-12-07|      20|          4906888|   2458|  240|            192|   5|     2|         6|\n|   12347.0| 2011-12-07|      16|          4906888|   2458|  240|            192|   7|     3|         7|\n|   12347.0| 2011-12-07|      12|          4906888|   2458|  240|            192|   8|     4|         8|\n|   12347.0| 2011-12-07|      12|          4906888|   2458|  240|            192|   8|     4|         9|\n|   12347.0| 2011-12-07|      10|          4906888|   2458|  240|            192|  10|     5|        10|\n|   12347.0| 2011-12-07|       6|          4906888|   2458|  240|            192|  11|     6|        11|\n|   12347.0| 2011-10-31|      48|          4906888|   2458|  240|            676|   1|     1|         1|\n|   12347.0| 2011-10-31|      36|          4906888|   2458|  240|            676|   2|     2|         2|\n|   12347.0| 2011-10-31|      36|          4906888|   2458|  240|            676|   2|     2|         3|\n|   12347.0| 2011-10-31|      24|          4906888|   2458|  240|            676|   4|     3|         4|\n|   12347.0| 2011-10-31|      24|          4906888|   2458|  240|            676|   4|     3|         5|\n|   12347.0| 2011-10-31|      24|          4906888|   2458|  240|            676|   4|     3|         6|\n|   12347.0| 2011-10-31|      24|          4906888|   2458|  240|            676|   4|     3|         7|\n+----------+-----------+--------+-----------------+-------+-----+---------------+----+------+----------+\nonly showing top 20 rows\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "from pyspark.sql.functions import col\n\ndf.where(\"CustomerId IS NOT NULL\")\\\n  .select(\n    col(\"CustomerId\"),\n    col(\"InvoiceDate\"),\n    col(\"Quantity\"),\n    overall_dataset_Q.alias(\"overall_dataset_Q\"),\n    total_Q.alias(\"total_Q\"),\n    max_Q.alias(\"max_Q\"),\n    total_Q_by_date.alias(\"total_Q_by_date\"),\n    rank.alias(\"rank\"),\n    d_rank.alias(\"d_rank\"),\n    row_number.alias(\"row_number\"))\\\n  .orderBy(\"CustomerId\", desc(\"InvoiceDate\"), \"row_number\").show()"}, {"cell_type": "markdown", "metadata": {}, "source": "## Rollups\n\nA rollup is a multidimensional aggregation that performs a variety of group-by style calculations for us.\n\nLet\u2019s create a rollup that looks across time (with our new `InvoiceDate` column) and space (with the `Country` column) and creates a new DataFrame that includes \n- the grand total over all dates \n- the grand total for each date in the DataFrame \n- and the subtotal for each country on each date in the DataFrame"}, {"cell_type": "code", "execution_count": 28, "metadata": {}, "outputs": [], "source": "dfNoNull = df.na.drop()\ndfNoNull.createOrReplaceTempView(\"dfNoNull\")"}, {"cell_type": "code", "execution_count": 29, "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "[Stage 67:=============================================>           (8 + 2) / 10]\r"}, {"name": "stdout", "output_type": "stream", "text": "+-----------+--------------+-------------+\n|InvoiceDate|       Country|sum(Quantity)|\n+-----------+--------------+-------------+\n|       NULL|          NULL|      4906888|\n| 2010-12-01|        Norway|         1852|\n| 2010-12-01|United Kingdom|        21167|\n| 2010-12-01|        France|          449|\n| 2010-12-01|          NULL|        24032|\n| 2010-12-01|       Germany|          117|\n| 2010-12-01|          EIRE|          243|\n| 2010-12-01|   Netherlands|           97|\n| 2010-12-01|     Australia|          107|\n| 2010-12-02|          EIRE|            4|\n+-----------+--------------+-------------+\nonly showing top 10 rows\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "rolledUpDF = dfNoNull.rollup(\"InvoiceDate\", \"Country\")\\\n  .agg(sum(\"Quantity\"))\\\n  .orderBy(\"InvoiceDate\")\n\nrolledUpDF.show(10)"}, {"cell_type": "markdown", "metadata": {}, "source": "Now where you see the null values is where you\u2019ll find the grand totals:"}, {"cell_type": "code", "execution_count": 30, "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "[Stage 70:===================================================>     (9 + 1) / 10]\r"}, {"name": "stdout", "output_type": "stream", "text": "+-----------+-------+-------------+\n|InvoiceDate|Country|sum(Quantity)|\n+-----------+-------+-------------+\n| 2010-12-01|   NULL|        24032|\n| 2010-12-02|   NULL|        20855|\n| 2010-12-03|   NULL|        11548|\n| 2010-12-05|   NULL|        16394|\n| 2010-12-06|   NULL|        16095|\n+-----------+-------+-------------+\nonly showing top 5 rows\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "rolledUpDF.where(\"Country IS NULL\").where(\"InvoiceDate IS NOT NULL\").show(5)"}, {"cell_type": "markdown", "metadata": {}, "source": "A null in both rollup columns specifies the grand total across both of those columns:"}, {"cell_type": "code", "execution_count": 31, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+-----------+-------+-------------+\n|InvoiceDate|Country|sum(Quantity)|\n+-----------+-------+-------------+\n|       NULL|   NULL|      4906888|\n+-----------+-------+-------------+\n\n"}], "source": "rolledUpDF.where(\"InvoiceDate IS NULL\").show()"}, {"cell_type": "markdown", "metadata": {}, "source": "## Cube\n\nA `cube` takes the `rollup` to a level deeper. Rather than treating elements hierarchically, a cube does the same thing across all dimensions. This means that it won\u2019t just go by date over the entire time period, but also the country. To pose this as a question again, can you make a table that includes the following?\n\n* The total across all dates and countries\n* The total for each date across all countries\n* The total for each country on each date\n* The total for each country across all dates\n\nThe method call is quite similar, but instead of calling `rollup`, we call `cube`:"}, {"cell_type": "code", "execution_count": 32, "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "[Stage 76:=======================================>                 (7 + 3) / 10]\r"}, {"name": "stdout", "output_type": "stream", "text": "+-----------+--------------+-------------+\n|InvoiceDate|       Country|sum(Quantity)|\n+-----------+--------------+-------------+\n|       NULL|         Japan|        25218|\n|       NULL|      Portugal|        16044|\n|       NULL|United Kingdom|      4008533|\n|       NULL|       Finland|        10666|\n|       NULL|Czech Republic|          592|\n+-----------+--------------+-------------+\nonly showing top 5 rows\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "from pyspark.sql.functions import sum\n\ncubbedDf = dfNoNull.cube(\"InvoiceDate\", \"Country\")\\\n  .agg(sum(\"Quantity\"))\\\n  .orderBy(\"InvoiceDate\")\n\ncubbedDf.show(5)"}, {"cell_type": "code", "execution_count": 33, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+-----------+-------+-------------+\n|InvoiceDate|Country|sum(Quantity)|\n+-----------+-------+-------------+\n|       NULL|   NULL|      4906888|\n| 2010-12-01|   NULL|        24032|\n| 2010-12-02|   NULL|        20855|\n| 2010-12-03|   NULL|        11548|\n| 2010-12-05|   NULL|        16394|\n+-----------+-------+-------------+\nonly showing top 5 rows\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "cubbedDf.where(\"Country IS NULL\").show(5)"}, {"cell_type": "markdown", "metadata": {}, "source": "It\u2019s a great way to create a quick summary table that others can use later on."}, {"cell_type": "markdown", "metadata": {}, "source": "## Pivot\n\nPivots make it possible for you to convert a row into a column. For example, in our current data we have a `Country` column. With a `pivot`, we can aggregate according to some function for each of those given countries and display them in an easy-to-query way:"}, {"cell_type": "code", "execution_count": 34, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "root\n |-- InvoiceNo: string (nullable = true)\n |-- StockCode: string (nullable = true)\n |-- Description: string (nullable = true)\n |-- Quantity: long (nullable = true)\n |-- InvoiceDate: date (nullable = true)\n |-- UnitPrice: double (nullable = true)\n |-- CustomerID: string (nullable = true)\n |-- Country: string (nullable = true)\n\n"}], "source": "df.printSchema()"}, {"cell_type": "code", "execution_count": 35, "metadata": {}, "outputs": [], "source": "pivoted = df.groupBy(\"InvoiceDate\").pivot(\"Country\").sum()"}, {"cell_type": "markdown", "metadata": {}, "source": "This DataFrame will now have a column for every combination of country, numeric variable, and a column specifying the date. For example, for USA we have the following columns: USA_sum(Quantity), and USA_sum(UnitPrice). This represents one for each numeric column in our dataset (because we just performed an aggregation over all of them).\n\nHere\u2019s an example query and result from this data:"}, {"cell_type": "code", "execution_count": 42, "metadata": {}, "outputs": [{"data": {"text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>InvoiceDate</th>\n      <th>Australia_sum(Quantity)</th>\n      <th>Australia_sum(UnitPrice)</th>\n      <th>Austria_sum(Quantity)</th>\n      <th>Austria_sum(UnitPrice)</th>\n      <th>Bahrain_sum(Quantity)</th>\n      <th>Bahrain_sum(UnitPrice)</th>\n      <th>Belgium_sum(Quantity)</th>\n      <th>Belgium_sum(UnitPrice)</th>\n      <th>Brazil_sum(Quantity)</th>\n      <th>...</th>\n      <th>Switzerland_sum(Quantity)</th>\n      <th>Switzerland_sum(UnitPrice)</th>\n      <th>USA_sum(Quantity)</th>\n      <th>USA_sum(UnitPrice)</th>\n      <th>United Arab Emirates_sum(Quantity)</th>\n      <th>United Arab Emirates_sum(UnitPrice)</th>\n      <th>United Kingdom_sum(Quantity)</th>\n      <th>United Kingdom_sum(UnitPrice)</th>\n      <th>Unspecified_sum(Quantity)</th>\n      <th>Unspecified_sum(UnitPrice)</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2010-12-01</td>\n      <td>107.0</td>\n      <td>73.9</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>23949</td>\n      <td>12428.08</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2011-04-06</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>7935</td>\n      <td>3592.40</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2011-08-30</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>10092</td>\n      <td>15480.07</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2011-04-27</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>17098</td>\n      <td>4042.97</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2011-08-05</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>9960</td>\n      <td>3631.15</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows \u00d7 77 columns</p>\n</div>", "text/plain": "  InvoiceDate  Australia_sum(Quantity)  Australia_sum(UnitPrice)  \\\n0  2010-12-01                    107.0                      73.9   \n1  2011-04-06                      NaN                       NaN   \n2  2011-08-30                      NaN                       NaN   \n3  2011-04-27                      NaN                       NaN   \n4  2011-08-05                      NaN                       NaN   \n\n   Austria_sum(Quantity)  Austria_sum(UnitPrice)  Bahrain_sum(Quantity)  \\\n0                    NaN                     NaN                    NaN   \n1                    NaN                     NaN                    NaN   \n2                    NaN                     NaN                    NaN   \n3                    NaN                     NaN                    NaN   \n4                    NaN                     NaN                    NaN   \n\n   Bahrain_sum(UnitPrice)  Belgium_sum(Quantity)  Belgium_sum(UnitPrice)  \\\n0                     NaN                    NaN                     NaN   \n1                     NaN                    NaN                     NaN   \n2                     NaN                    NaN                     NaN   \n3                     NaN                    NaN                     NaN   \n4                     NaN                    NaN                     NaN   \n\n   Brazil_sum(Quantity)  ...  Switzerland_sum(Quantity)  \\\n0                   NaN  ...                        NaN   \n1                   NaN  ...                        NaN   \n2                   NaN  ...                        NaN   \n3                   NaN  ...                        NaN   \n4                   NaN  ...                        NaN   \n\n   Switzerland_sum(UnitPrice)  USA_sum(Quantity)  USA_sum(UnitPrice)  \\\n0                         NaN                NaN                 NaN   \n1                         NaN                NaN                 NaN   \n2                         NaN                NaN                 NaN   \n3                         NaN                NaN                 NaN   \n4                         NaN                NaN                 NaN   \n\n   United Arab Emirates_sum(Quantity)  United Arab Emirates_sum(UnitPrice)  \\\n0                                 NaN                                  NaN   \n1                                 NaN                                  NaN   \n2                                 NaN                                  NaN   \n3                                 NaN                                  NaN   \n4                                 NaN                                  NaN   \n\n   United Kingdom_sum(Quantity)  United Kingdom_sum(UnitPrice)  \\\n0                         23949                       12428.08   \n1                          7935                        3592.40   \n2                         10092                       15480.07   \n3                         17098                        4042.97   \n4                          9960                        3631.15   \n\n   Unspecified_sum(Quantity)  Unspecified_sum(UnitPrice)  \n0                        NaN                         NaN  \n1                        NaN                         NaN  \n2                        NaN                         NaN  \n3                        NaN                         NaN  \n4                        NaN                         NaN  \n\n[5 rows x 77 columns]"}, "execution_count": 42, "metadata": {}, "output_type": "execute_result"}], "source": "pivoted.limit(5).toPandas()"}, {"cell_type": "code", "execution_count": 43, "metadata": {}, "outputs": [{"data": {"text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>InvoiceDate</th>\n      <th>Australia_sum(Quantity)</th>\n      <th>Australia_sum(UnitPrice)</th>\n      <th>Austria_sum(Quantity)</th>\n      <th>Austria_sum(UnitPrice)</th>\n      <th>Bahrain_sum(Quantity)</th>\n      <th>Bahrain_sum(UnitPrice)</th>\n      <th>Belgium_sum(Quantity)</th>\n      <th>Belgium_sum(UnitPrice)</th>\n      <th>Brazil_sum(Quantity)</th>\n      <th>...</th>\n      <th>Switzerland_sum(Quantity)</th>\n      <th>Switzerland_sum(UnitPrice)</th>\n      <th>USA_sum(Quantity)</th>\n      <th>USA_sum(UnitPrice)</th>\n      <th>United Arab Emirates_sum(Quantity)</th>\n      <th>United Arab Emirates_sum(UnitPrice)</th>\n      <th>United Kingdom_sum(Quantity)</th>\n      <th>United Kingdom_sum(UnitPrice)</th>\n      <th>Unspecified_sum(Quantity)</th>\n      <th>Unspecified_sum(UnitPrice)</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2011-12-09</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>203.0</td>\n      <td>32.22</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>9534</td>\n      <td>8713.70</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2011-12-06</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>396.0</td>\n      <td>199.26</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>27191</td>\n      <td>9512.65</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2011-12-08</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>148.0</td>\n      <td>64.58</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>-196.0</td>\n      <td>13.75</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>32576</td>\n      <td>21259.84</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2011-12-07</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>27611</td>\n      <td>7501.39</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n<p>4 rows \u00d7 77 columns</p>\n</div>", "text/plain": "  InvoiceDate  Australia_sum(Quantity)  Australia_sum(UnitPrice)  \\\n0  2011-12-09                      NaN                       NaN   \n1  2011-12-06                      NaN                       NaN   \n2  2011-12-08                      NaN                       NaN   \n3  2011-12-07                      NaN                       NaN   \n\n   Austria_sum(Quantity)  Austria_sum(UnitPrice)  Bahrain_sum(Quantity)  \\\n0                    NaN                     NaN                    NaN   \n1                    NaN                     NaN                    NaN   \n2                  148.0                   64.58                    NaN   \n3                    NaN                     NaN                    NaN   \n\n   Bahrain_sum(UnitPrice)  Belgium_sum(Quantity)  Belgium_sum(UnitPrice)  \\\n0                     NaN                  203.0                   32.22   \n1                     NaN                  396.0                  199.26   \n2                     NaN                    NaN                     NaN   \n3                     NaN                    NaN                     NaN   \n\n   Brazil_sum(Quantity)  ...  Switzerland_sum(Quantity)  \\\n0                   NaN  ...                        NaN   \n1                   NaN  ...                        NaN   \n2                   NaN  ...                        NaN   \n3                   NaN  ...                        NaN   \n\n   Switzerland_sum(UnitPrice)  USA_sum(Quantity)  USA_sum(UnitPrice)  \\\n0                         NaN                NaN                 NaN   \n1                         NaN                NaN                 NaN   \n2                         NaN             -196.0               13.75   \n3                         NaN                NaN                 NaN   \n\n   United Arab Emirates_sum(Quantity)  United Arab Emirates_sum(UnitPrice)  \\\n0                                 NaN                                  NaN   \n1                                 NaN                                  NaN   \n2                                 NaN                                  NaN   \n3                                 NaN                                  NaN   \n\n   United Kingdom_sum(Quantity)  United Kingdom_sum(UnitPrice)  \\\n0                          9534                        8713.70   \n1                         27191                        9512.65   \n2                         32576                       21259.84   \n3                         27611                        7501.39   \n\n   Unspecified_sum(Quantity)  Unspecified_sum(UnitPrice)  \n0                        NaN                         NaN  \n1                        NaN                         NaN  \n2                        NaN                         NaN  \n3                        NaN                         NaN  \n\n[4 rows x 77 columns]"}, "execution_count": 43, "metadata": {}, "output_type": "execute_result"}], "source": "pivoted.where(\"InvoiceDate > '2011-12-05'\").limit(5).toPandas()"}, {"cell_type": "code", "execution_count": 38, "metadata": {"tags": []}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+-----------+----------------------------+\n|InvoiceDate|United Kingdom_sum(Quantity)|\n+-----------+----------------------------+\n| 2011-12-09|                        9534|\n| 2011-12-02|                       24457|\n| 2011-12-04|                       10816|\n| 2011-12-05|                       42414|\n| 2011-12-06|                       27191|\n| 2011-12-08|                       32576|\n| 2011-12-07|                       27611|\n+-----------+----------------------------+\n\n"}], "source": "pivoted.where(\"InvoiceDate > '2011-12-01'\").select(\"InvoiceDate\" ,\"United Kingdom_sum(Quantity)\").show()"}], "metadata": {"kernelspec": {"display_name": "PySpark", "language": "python", "name": "pyspark"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.11.8"}}, "nbformat": 4, "nbformat_minor": 4}