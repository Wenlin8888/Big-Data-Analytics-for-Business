{"cells": [{"cell_type": "markdown", "metadata": {}, "source": "# Regression\n\nRegression is a logical extension of classification. Rather than just predicting a single value from a set of values, regression is the act of predicting a real number (or continuous variable) from a set of features (represented as numbers).\n\nRegression can be harder than classification because, from a mathematical perspective, there are an infinite number of possible output values. Furthermore, we aim to optimize some metric of error between the predicted and true value, as opposed to an accuracy rate. Aside from that, regression and classification are fairly similar. For this reason, we will see a lot of the same underlying concepts applied to regression as we did with classification.\n\n## Use Cases\n\nThe following is a small set of regression use cases that can get you thinking about potential regression problems in your own domain:\n\n* Predicting movie viewership: Given information about a movie and the movie-going public, such as how many people have watched the trailer or shared it on social media, you might want to predict how many people are likely to watch the movie when it comes out.\n\n* Predicting company revenue: Given a current growth trajectory, the market, and seasonality, you might want to predict how much revenue a company will gain in the future.\n\n* Predicting crop yield: Given information about the particular area in which a crop is grown, as well as the current weather throughout the year, you might want to predict the total crop yield for a particular plot of land."}, {"cell_type": "markdown", "metadata": {}, "source": "## Regression Models in MLlib\n\nThere are several fundamental regression models in MLlib. This list is current as of Spark 2.4 but will grow:\n\n* [Linear regression](https://spark.apache.org/docs/latest/ml-classification-regression.html#linear-regression)\n* [Generalized linear regression](https://spark.apache.org/docs/latest/ml-classification-regression.html#generalized-linear-regression)\n* [Decision tree regression](https://spark.apache.org/docs/latest/ml-classification-regression.html#decision-tree-regression)\n* [Random forest regression](https://spark.apache.org/docs/latest/ml-classification-regression.html#random-forest-regression)\n* [Gradient-boosted tree regression](https://spark.apache.org/docs/latest/ml-classification-regression.html#gradient-boosted-tree-regression)\n* [Survival regression](https://spark.apache.org/docs/latest/ml-classification-regression.html#survival-regression)\n* [Isotonic regression](https://spark.apache.org/docs/latest/ml-classification-regression.html#isotonic-regression)"}, {"cell_type": "markdown", "metadata": {}, "source": "Here we will cover:\n* A simple explanation of a few of these models and the intuition behind their algorithms\n* Model hyperparameters (the different ways that we can initialize the model)\n* Training parameters (parameters that affect how the model is trained)\n* Prediction parameters (parameters that affect how predictions are made)\n\nYou can search over the hyperparameters and training parameters using a `ParamGrid` as we saw before.\n\n## Model Scalability\n\nThe regression models in MLlib all scale to large datasets. Table below is a simple model scalability scorecard that will help you in choosing the best model for your particular task (if scalability is your core consideration). These will depend on your configuration, machine size, and other factors.\n\n|Model\t|Number features|\tTraining examples|\n|--|--|--|\n|Linear regression|1 to 10 million|No limit|\n|Generalized linear regression|4,096|No limit|\n|Decision trees|1,000s|No limit|\n|Random forest|10,000s|No limit|\n|Gradient-boosted trees|1,000s|No limit|\n|Survival regression|1 to 10 million|No limit|\n|Isotonic regression|N/A|Millions|"}, {"cell_type": "code", "execution_count": 1, "metadata": {}, "outputs": [], "source": "%matplotlib inline\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\n\nfrom IPython.display import Pretty as disp\nhint = 'https://raw.githubusercontent.com/soltaniehha/Big-Data-Analytics-for-Business/master/docs/hints/'  # path to hints on GitHub"}, {"cell_type": "markdown", "metadata": {}, "source": "Let\u2019s read in some sample data that we will use throughout the notebook:"}, {"cell_type": "code", "execution_count": 2, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "gs://is843-demo/notebooks/jupyter/data/\n"}], "source": "# the following line gets the bucket name attached to our cluster\nbucket = spark._jsc.hadoopConfiguration().get(\"fs.gs.system.bucket\")\n\n# specifying the path to our bucket where the data is located (no need to edit this path anymore)\ndata = \"gs://\" + bucket + \"/notebooks/jupyter/data/\"\nprint(data)"}, {"cell_type": "code", "execution_count": 3, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "sales datasets consists of 5 rows.\n+--------------+-----+\n|      features|label|\n+--------------+-----+\n|[3.0,10.1,3.0]|  2.0|\n| [2.0,1.1,1.0]|  1.0|\n|[1.0,0.1,-1.0]|  0.0|\n|[1.0,0.1,-1.0]|  0.0|\n| [2.0,4.1,1.0]|  2.0|\n+--------------+-----+\n\n"}], "source": "small_df = spark.read.load(data + \"regression\")\n\nsmall_df.cache()\nprint(\"sales datasets consists of {} rows.\".format(small_df.count()))\nsmall_df.show()"}, {"cell_type": "markdown", "metadata": {}, "source": "## Linear Regression\n\nLinear regression assumes that a linear combination of your input features (the sum of each feature multiplied by a weight) results along with an amount of Gaussian error in the output. This linear assumption (along with Gaussian error) does not always hold true, but it does make for a simple, interpretable model that\u2019s hard to overfit. Like logistic regression, Spark implements `ElasticNet` regularization for this, allowing you to mix *L1* and *L2* regularization.\n\n### Model Hyperparameters\n\nLinear regression has the same model hyperparameters as logistic regression. See below:\n\nModel hyperparameters are configurations that determine the basic structure of the model itself. The following hyperparameters are available for linear regression:\n\n**`elasticNetParam`** (default: 0.0)\n\nThe ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty. \n\n**`epsilon`** (default: 1.35)\n\nThe shape parameter to control the amount of robustness. Must be > 1.0. Only valid when loss is huber\n\n**`featuresCol`** (default: features)\n\nFeatures column name.\n\n**`labelCol`** (default: label)\n\nLabel column name. \n\n**`predictionCol`** (default: prediction)\n\nPrediction column name.\n\n\n**`loss`** (default: squaredError)\n\nThe loss function to be optimized. Supported options: squaredError, huber.\n\n**`maxBlockSizeInMB`** (default: 0.0)\n\nMaximum memory in MB for stacking input data into blocks. Data is stacked within partitions. If more than remaining data size in a partition then it is adjusted to the data size. Default 0.0 represents choosing optimal value, depends on specific algorithm. Must be >= 0.\n\n\n**`fitIntercept`** (default: True)\n\nWhether to fit an intercept term.\n\n**`regParam`** (default: 0.0)\n\nA value \u2265 0. that determines how much weight to give to the regularization term in the objective function. Choosing a value here is again going to be a function of noise and dimensionality in our dataset. In a pipeline, try a wide range of values (e.g., 0, 0.01, 0.1, 1).\n\n**`solver`** (default: auto)\n\nThe solver algorithm for optimization. Supported options: auto, normal, l-bfgs.\n\n**`standardization`** (default: True)\n\nCan be true or false, whether or not to standardize the inputs before passing them into the model.\n\n### Training Parameters\n\nLinear regression also shares all of the same training parameters from logistic regression. See below:\n\nTraining parameters are used to specify how we perform our training. Here are the training parameters for logistic regression.\n\n**`maxIter`** (default: 100)\n\nMax number of iterations (>= 0).\n\n\n**`tol`** (default: 1e-06)\n\nThis convergence tolerance specifies a threshold by which changes in parameters show that we optimized our weights enough, and can stop iterating. It lets the algorithm stop before `maxIter` iterations. The default value is 1.0E-6. This also shouldn\u2019t be the first parameter you look to tune.\n\n**`weightCol`** (undefined)\n\nThe name of a weight column used to weigh certain rows more than others. This can be a useful tool if you have some other measure of how important a particular training example is and have a weight associated with it. For example, you might have 10,000 examples where you know that some labels are more accurate than others. You can weigh the labels you know are correct more than the ones you don\u2019t. If this is not set or empty, we treat all instance weights as 1.0.\n\n### Example\n\nHere\u2019s a short example of using linear regression on our sample dataset:"}, {"cell_type": "code", "execution_count": 4, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "aggregationDepth: suggested depth for treeAggregate (>= 2). (default: 2)\nelasticNetParam: the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty. (default: 0.0, current: 0.8)\nepsilon: The shape parameter to control the amount of robustness. Must be > 1.0. Only valid when loss is huber (default: 1.35)\nfeaturesCol: features column name. (default: features)\nfitIntercept: whether to fit an intercept term. (default: True)\nlabelCol: label column name. (default: label)\nloss: The loss function to be optimized. Supported options: squaredError, huber. (default: squaredError)\nmaxIter: max number of iterations (>= 0). (default: 100, current: 10)\npredictionCol: prediction column name. (default: prediction)\nregParam: regularization parameter (>= 0). (default: 0.0, current: 0.3)\nsolver: The solver algorithm for optimization. Supported options: auto, normal, l-bfgs. (default: auto)\nstandardization: whether to standardize the training features before fitting the model. (default: True)\ntol: the convergence tolerance for iterative algorithms (>= 0). (default: 1e-06)\nweightCol: weight column name. If this is not set or empty, we treat all instance weights as 1.0. (undefined)\n"}], "source": "from pyspark.ml.regression import LinearRegression\n\nlr = LinearRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8)\nprint(lr.explainParams())"}, {"cell_type": "code", "execution_count": 5, "metadata": {}, "outputs": [], "source": "lrModel = lr.fit(small_df)"}, {"cell_type": "markdown", "metadata": {}, "source": "### Training Summary\n\nJust as in logistic regression, we get detailed training information back from our model. The summary method returns a summary object with several fields. Let\u2019s go through these in turn. \n\n* The residuals are simply the weights for each of the features that we input into the model. \n* The objective history shows how our training is going at every iteration. \n* The root mean squared error is a measure of how well our line is fitting the data, determined by looking at the distance between each predicted value and the actual value in the data. \n* The R-squared variable is a measure of the proportion of the variance of the predicted variable that is captured by the model.\n\nThere are a number of metrics and summary information that may be relevant to your use case. This section demonstrates the API, but does not comprehensively cover every metric (consult the API documentation for more information).\n\nPrint the coefficients and intercept for linear regression:"}, {"cell_type": "code", "execution_count": 6, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Coefficients: [0.36272077849971757,0.0002028093920829202,0.18136038924985703]\nIntercept: 0.2376576560351371\n"}], "source": "print(\"Coefficients: %s\" % str(lrModel.coefficients))\nprint(\"Intercept: %s\" % str(lrModel.intercept))"}, {"cell_type": "markdown", "metadata": {}, "source": "Summarize the model over the training set and print out some metrics:"}, {"cell_type": "code", "execution_count": 7, "metadata": {}, "outputs": [], "source": "trainingSummary = lrModel.summary"}, {"cell_type": "code", "execution_count": 8, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "RMSE: 0.473084\nr2: 0.720239\nnumIterations: 6\nobjectiveHistory: [0.5000000000000001, 0.4315295810362787, 0.3132335933881022, 0.31225692666554117, 0.309150608198303, 0.30915058933480255]\n"}], "source": "print(\"RMSE: %f\" % trainingSummary.rootMeanSquaredError)\nprint(\"r2: %f\" % trainingSummary.r2)\nprint(\"numIterations: %d\" % trainingSummary.totalIterations)\nprint(\"objectiveHistory: %s\" % str(trainingSummary.objectiveHistory))"}, {"cell_type": "markdown", "metadata": {}, "source": "Residuals:"}, {"cell_type": "code", "execution_count": 9, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+--------------------+\n|           residuals|\n+--------------------+\n| 0.12805046585610147|\n|-0.14468269261572053|\n|-0.41903832622420595|\n|-0.41903832622420595|\n|  0.8547088792080308|\n+--------------------+\n\n"}], "source": "trainingSummary.residuals.show()"}, {"cell_type": "markdown", "metadata": {}, "source": "# Your turn\n\nUsing a new dataset find a linear regression model that fits the data best.\n\nDataset: [Online News Popularity Data Set](https://archive.ics.uci.edu/ml/datasets/Online+News+Popularity) from UC Irvine datasets\n\nAttribute Information:\n\nNumber of Attributes: 61 (58 predictive attributes, 2 non-predictive, 1 goal field) \n\nAttribute Information: \n\n0. url: URL of the article (non-predictive) \n1. timedelta: Days between the article publication and the dataset acquisition (non-predictive) \n2. n_tokens_title: Number of words in the title \n3. n_tokens_content: Number of words in the content \n4. n_unique_tokens: Rate of unique words in the content \n5. n_non_stop_words: Rate of non-stop words in the content \n6. n_non_stop_unique_tokens: Rate of unique non-stop words in the content \n7. num_hrefs: Number of links \n8. num_self_hrefs: Number of links to other articles published by Mashable \n9. num_imgs: Number of images \n10. num_videos: Number of videos \n11. average_token_length: Average length of the words in the content \n12. num_keywords: Number of keywords in the metadata \n13. data_channel_is_lifestyle: Is data channel 'Lifestyle'? \n14. data_channel_is_entertainment: Is data channel 'Entertainment'? \n15. data_channel_is_bus: Is data channel 'Business'? \n16. data_channel_is_socmed: Is data channel 'Social Media'? \n17. data_channel_is_tech: Is data channel 'Tech'? \n18. data_channel_is_world: Is data channel 'World'? \n19. kw_min_min: Worst keyword (min. shares) \n20. kw_max_min: Worst keyword (max. shares) \n21. kw_avg_min: Worst keyword (avg. shares) \n22. kw_min_max: Best keyword (min. shares) \n23. kw_max_max: Best keyword (max. shares) \n24. kw_avg_max: Best keyword (avg. shares) \n25. kw_min_avg: Avg. keyword (min. shares) \n26. kw_max_avg: Avg. keyword (max. shares) \n27. kw_avg_avg: Avg. keyword (avg. shares) \n28. self_reference_min_shares: Min. shares of referenced articles in Mashable \n29. self_reference_max_shares: Max. shares of referenced articles in Mashable \n30. self_reference_avg_sharess: Avg. shares of referenced articles in Mashable \n31. weekday_is_monday: Was the article published on a Monday? \n32. weekday_is_tuesday: Was the article published on a Tuesday? \n33. weekday_is_wednesday: Was the article published on a Wednesday? \n34. weekday_is_thursday: Was the article published on a Thursday? \n35. weekday_is_friday: Was the article published on a Friday? \n36. weekday_is_saturday: Was the article published on a Saturday? \n37. weekday_is_sunday: Was the article published on a Sunday? \n38. is_weekend: Was the article published on the weekend? \n39. LDA_00: Closeness to LDA topic 0 \n40. LDA_01: Closeness to LDA topic 1 \n41. LDA_02: Closeness to LDA topic 2 \n42. LDA_03: Closeness to LDA topic 3 \n43. LDA_04: Closeness to LDA topic 4 \n44. global_subjectivity: Text subjectivity \n45. global_sentiment_polarity: Text sentiment polarity \n46. global_rate_positive_words: Rate of positive words in the content \n47. global_rate_negative_words: Rate of negative words in the content \n48. rate_positive_words: Rate of positive words among non-neutral tokens \n49. rate_negative_words: Rate of negative words among non-neutral tokens \n50. avg_positive_polarity: Avg. polarity of positive words \n51. min_positive_polarity: Min. polarity of positive words \n52. max_positive_polarity: Max. polarity of positive words \n53. avg_negative_polarity: Avg. polarity of negative words \n54. min_negative_polarity: Min. polarity of negative words \n55. max_negative_polarity: Max. polarity of negative words \n56. title_subjectivity: Title subjectivity \n57. title_sentiment_polarity: Title polarity \n58. abs_title_subjectivity: Absolute subjectivity level \n59. abs_title_sentiment_polarity: Absolute polarity level \n60. shares: Number of shares (target)\n\nData can be found in course's GitHub: **data/OnlineNewsPopularity.csv**"}, {"cell_type": "markdown", "metadata": {}, "source": "First upload the data to your Google Cloud Storage and then load it to a PySpark DataFrame. Then show the first couple of rows and its schema. You can also cache it for a better performance:"}, {"cell_type": "code", "execution_count": 10, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+--------------------+---------+--------------+----------------+---------------+----------------+------------------------+---------+--------------+--------+----------+--------------------+------------+-------------------------+-----------------------------+-------------------+----------------------+--------------------+---------------------+----------+----------+----------+----------+----------+----------+----------+----------+----------+-------------------------+-------------------------+--------------------------+-----------------+------------------+--------------------+-------------------+-----------------+-------------------+-----------------+----------+--------------+--------------+---------------+---------------+---------------+-------------------+-------------------------+--------------------------+--------------------------+-------------------+-------------------+---------------------+---------------------+---------------------+---------------------+---------------------+---------------------+------------------+------------------------+----------------------+----------------------------+------+\n|                 url|timedelta|n_tokens_title|n_tokens_content|n_unique_tokens|n_non_stop_words|n_non_stop_unique_tokens|num_hrefs|num_self_hrefs|num_imgs|num_videos|average_token_length|num_keywords|data_channel_is_lifestyle|data_channel_is_entertainment|data_channel_is_bus|data_channel_is_socmed|data_channel_is_tech|data_channel_is_world|kw_min_min|kw_max_min|kw_avg_min|kw_min_max|kw_max_max|kw_avg_max|kw_min_avg|kw_max_avg|kw_avg_avg|self_reference_min_shares|self_reference_max_shares|self_reference_avg_sharess|weekday_is_monday|weekday_is_tuesday|weekday_is_wednesday|weekday_is_thursday|weekday_is_friday|weekday_is_saturday|weekday_is_sunday|is_weekend|        LDA_00|        LDA_01|         LDA_02|         LDA_03|         LDA_04|global_subjectivity|global_sentiment_polarity|global_rate_positive_words|global_rate_negative_words|rate_positive_words|rate_negative_words|avg_positive_polarity|min_positive_polarity|max_positive_polarity|avg_negative_polarity|min_negative_polarity|max_negative_polarity|title_subjectivity|title_sentiment_polarity|abs_title_subjectivity|abs_title_sentiment_polarity|shares|\n+--------------------+---------+--------------+----------------+---------------+----------------+------------------------+---------+--------------+--------+----------+--------------------+------------+-------------------------+-----------------------------+-------------------+----------------------+--------------------+---------------------+----------+----------+----------+----------+----------+----------+----------+----------+----------+-------------------------+-------------------------+--------------------------+-----------------+------------------+--------------------+-------------------+-----------------+-------------------+-----------------+----------+--------------+--------------+---------------+---------------+---------------+-------------------+-------------------------+--------------------------+--------------------------+-------------------+-------------------+---------------------+---------------------+---------------------+---------------------+---------------------+---------------------+------------------+------------------------+----------------------+----------------------------+------+\n|http://mashable.c...|    731.0|          12.0|           219.0| 0.663594466988|  0.999999992308|          0.815384609112|      4.0|           2.0|     1.0|       0.0|        4.6803652968|         5.0|                      0.0|                          1.0|                0.0|                   0.0|                 0.0|                  0.0|       0.0|       0.0|       0.0|       0.0|       0.0|       0.0|       0.0|       0.0|       0.0|                    496.0|                    496.0|                     496.0|              1.0|               0.0|                 0.0|                0.0|              0.0|                0.0|              0.0|       0.0|0.500331204081|0.378278929586|0.0400046751006|0.0412626477296|0.0401225435029|     0.521617145481|          0.0925619834711|           0.0456621004566|            0.013698630137|     0.769230769231|     0.230769230769|       0.378636363636|                  0.1|                  0.7|                -0.35|                 -0.6|                 -0.2|               0.5|                 -0.1875|                   0.0|                      0.1875| 593.0|\n+--------------------+---------+--------------+----------------+---------------+----------------+------------------------+---------+--------------+--------+----------+--------------------+------------+-------------------------+-----------------------------+-------------------+----------------------+--------------------+---------------------+----------+----------+----------+----------+----------+----------+----------+----------+----------+-------------------------+-------------------------+--------------------------+-----------------+------------------+--------------------+-------------------+-----------------+-------------------+-----------------+----------+--------------+--------------+---------------+---------------+---------------+-------------------+-------------------------+--------------------------+--------------------------+-------------------+-------------------+---------------------+---------------------+---------------------+---------------------+---------------------+---------------------+------------------+------------------------+----------------------+----------------------------+------+\nonly showing top 1 row\n\nroot\n |-- url: string (nullable = true)\n |-- timedelta: double (nullable = true)\n |-- n_tokens_title: double (nullable = true)\n |-- n_tokens_content: double (nullable = true)\n |-- n_unique_tokens: double (nullable = true)\n |-- n_non_stop_words: double (nullable = true)\n |-- n_non_stop_unique_tokens: double (nullable = true)\n |-- num_hrefs: double (nullable = true)\n |-- num_self_hrefs: double (nullable = true)\n |-- num_imgs: double (nullable = true)\n |-- num_videos: double (nullable = true)\n |-- average_token_length: double (nullable = true)\n |-- num_keywords: double (nullable = true)\n |-- data_channel_is_lifestyle: double (nullable = true)\n |-- data_channel_is_entertainment: double (nullable = true)\n |-- data_channel_is_bus: double (nullable = true)\n |-- data_channel_is_socmed: double (nullable = true)\n |-- data_channel_is_tech: double (nullable = true)\n |-- data_channel_is_world: double (nullable = true)\n |-- kw_min_min: double (nullable = true)\n |-- kw_max_min: double (nullable = true)\n |-- kw_avg_min: double (nullable = true)\n |-- kw_min_max: double (nullable = true)\n |-- kw_max_max: double (nullable = true)\n |-- kw_avg_max: double (nullable = true)\n |-- kw_min_avg: double (nullable = true)\n |-- kw_max_avg: double (nullable = true)\n |-- kw_avg_avg: double (nullable = true)\n |-- self_reference_min_shares: double (nullable = true)\n |-- self_reference_max_shares: double (nullable = true)\n |-- self_reference_avg_sharess: double (nullable = true)\n |-- weekday_is_monday: double (nullable = true)\n |-- weekday_is_tuesday: double (nullable = true)\n |-- weekday_is_wednesday: double (nullable = true)\n |-- weekday_is_thursday: double (nullable = true)\n |-- weekday_is_friday: double (nullable = true)\n |-- weekday_is_saturday: double (nullable = true)\n |-- weekday_is_sunday: double (nullable = true)\n |-- is_weekend: double (nullable = true)\n |-- LDA_00: double (nullable = true)\n |-- LDA_01: double (nullable = true)\n |-- LDA_02: double (nullable = true)\n |-- LDA_03: double (nullable = true)\n |-- LDA_04: double (nullable = true)\n |-- global_subjectivity: double (nullable = true)\n |-- global_sentiment_polarity: double (nullable = true)\n |-- global_rate_positive_words: double (nullable = true)\n |-- global_rate_negative_words: double (nullable = true)\n |-- rate_positive_words: double (nullable = true)\n |-- rate_negative_words: double (nullable = true)\n |-- avg_positive_polarity: double (nullable = true)\n |-- min_positive_polarity: double (nullable = true)\n |-- max_positive_polarity: double (nullable = true)\n |-- avg_negative_polarity: double (nullable = true)\n |-- min_negative_polarity: double (nullable = true)\n |-- max_negative_polarity: double (nullable = true)\n |-- title_subjectivity: double (nullable = true)\n |-- title_sentiment_polarity: double (nullable = true)\n |-- abs_title_subjectivity: double (nullable = true)\n |-- abs_title_sentiment_polarity: double (nullable = true)\n |-- shares: double (nullable = true)\n\nThis datasets consists of 39644 rows.\n"}], "source": "# Your answer goes here\n"}, {"cell_type": "code", "execution_count": 11, "metadata": {}, "outputs": [], "source": "# SOLUTION: Uncomment and execute the cell below to get help\n#disp(hint + '12-01-load')"}, {"cell_type": "markdown", "metadata": {}, "source": "Define an RFormula that uses all of the columns as features except the one(s) that are not numerical and call it `supervised`:"}, {"cell_type": "code", "execution_count": 12, "metadata": {}, "outputs": [], "source": "# Your answer goes here\n"}, {"cell_type": "code", "execution_count": 13, "metadata": {}, "outputs": [], "source": "# SOLUTION: Uncomment and execute the cell below to get help\n#disp(hint + '12-01-RFormula')"}, {"cell_type": "markdown", "metadata": {}, "source": "Fit the RFormula transformer and call it `fittedRF`:"}, {"cell_type": "code", "execution_count": 14, "metadata": {}, "outputs": [], "source": "# Your answer goes here\n"}, {"cell_type": "code", "execution_count": 15, "metadata": {}, "outputs": [], "source": "# SOLUTION: Uncomment and execute the cell below to get help\n#disp(hint + '12-01-fittedRF')"}, {"cell_type": "markdown", "metadata": {}, "source": "Using `fittedRF` transform our `df` DataFrame. Call this `preparedDF`:"}, {"cell_type": "code", "execution_count": 16, "metadata": {}, "outputs": [], "source": "# Your answer goes here\n"}, {"cell_type": "code", "execution_count": 17, "metadata": {}, "outputs": [], "source": "# SOLUTION: Uncomment and execute the cell below to get help\n#disp(hint + '12-01-preparedDF')"}, {"cell_type": "markdown", "metadata": {}, "source": "Print the first couple of rows of `preparedDF`:"}, {"cell_type": "code", "execution_count": 18, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----+\n|features                                                                                                                                                                                                                                                                                                                                                                                                                                                            |label|\n+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----+\n|(59,[0,1,2,3,4,5,6,7,8,10,11,13,27,28,29,30,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,58],[731.0,12.0,219.0,0.663594466988,0.999999992308,0.815384609112,4.0,2.0,1.0,4.6803652968,5.0,1.0,496.0,496.0,496.0,1.0,0.500331204081,0.378278929586,0.0400046751006,0.0412626477296,0.0401225435029,0.521617145481,0.0925619834711,0.0456621004566,0.013698630137,0.769230769231,0.230769230769,0.378636363636,0.1,0.7,-0.35,-0.6,-0.2,0.5,-0.1875,0.1875])|593.0|\n|(59,[0,1,2,3,4,5,6,7,8,10,11,14,30,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,57],[731.0,9.0,255.0,0.604743080614,0.999999993289,0.79194630341,3.0,1.0,1.0,4.9137254902,4.0,1.0,1.0,0.799755687423,0.0500466753998,0.0500962518137,0.0501006734234,0.0500007119405,0.341245791246,0.148947811448,0.043137254902,0.0156862745098,0.733333333333,0.266666666667,0.286914600551,0.0333333333333,0.7,-0.11875,-0.125,-0.1,0.5])                                 |711.0|\n+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----+\nonly showing top 2 rows\n\n"}], "source": "preparedDF.select('features', 'label').show(2, False)"}, {"cell_type": "markdown", "metadata": {}, "source": "Below we will retrieve the name of the columns used to make our feature vector and store them in a pandas DataFrame. Notice that we don't have any binary terms, so we have dropped that from our metadata extraction:"}, {"cell_type": "code", "execution_count": 19, "metadata": {}, "outputs": [{"data": {"text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>name</th>\n    </tr>\n    <tr>\n      <th>idx</th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>timedelta</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>n_tokens_title</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>n_tokens_content</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>n_unique_tokens</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>n_non_stop_words</td>\n    </tr>\n  </tbody>\n</table>\n</div>", "text/plain": "                 name\nidx                  \n0           timedelta\n1      n_tokens_title\n2    n_tokens_content\n3     n_unique_tokens\n4    n_non_stop_words"}, "execution_count": 19, "metadata": {}, "output_type": "execute_result"}], "source": "featureCols = pd.DataFrame(preparedDF.schema[\"features\"].metadata[\"ml_attr\"][\"attrs\"][\"numeric\"]).sort_values(\"idx\")\n\nfeatureCols = featureCols.set_index('idx')\nfeatureCols.head()"}, {"cell_type": "markdown", "metadata": {}, "source": "Instantiate a linear regression, call it `lr`:"}, {"cell_type": "code", "execution_count": 20, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "aggregationDepth: suggested depth for treeAggregate (>= 2). (default: 2)\nelasticNetParam: the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty. (default: 0.0, current: 0.8)\nepsilon: The shape parameter to control the amount of robustness. Must be > 1.0. Only valid when loss is huber (default: 1.35)\nfeaturesCol: features column name. (default: features)\nfitIntercept: whether to fit an intercept term. (default: True)\nlabelCol: label column name. (default: label)\nloss: The loss function to be optimized. Supported options: squaredError, huber. (default: squaredError)\nmaxIter: max number of iterations (>= 0). (default: 100, current: 100)\npredictionCol: prediction column name. (default: prediction)\nregParam: regularization parameter (>= 0). (default: 0.0, current: 0.3)\nsolver: The solver algorithm for optimization. Supported options: auto, normal, l-bfgs. (default: auto)\nstandardization: whether to standardize the training features before fitting the model. (default: True)\ntol: the convergence tolerance for iterative algorithms (>= 0). (default: 1e-06)\nweightCol: weight column name. If this is not set or empty, we treat all instance weights as 1.0. (undefined)\n"}], "source": "# Your answer goes here\n"}, {"cell_type": "code", "execution_count": 21, "metadata": {}, "outputs": [], "source": "# SOLUTION: Uncomment and execute the cell below to get help\n#disp(hint + '12-01-lr')"}, {"cell_type": "markdown", "metadata": {}, "source": "Fit the model using `preparedDF`. Call this model `lrModel`: "}, {"cell_type": "code", "execution_count": 22, "metadata": {}, "outputs": [], "source": "# Your answer goes here\n"}, {"cell_type": "code", "execution_count": 23, "metadata": {}, "outputs": [], "source": "# SOLUTION: Uncomment and execute the cell below to get help\n#disp(hint + '12-01-lrModel')"}, {"cell_type": "markdown", "metadata": {}, "source": "* Print coefficients\n* Using the model summary output some of its components and assess the goodness of fit"}, {"cell_type": "code", "execution_count": 24, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Coefficients: [1.7482777166102026,112.74777422130794,0.3318256783212383,166.7680149618438,-150.51591165325604,70.55017886287425,26.773631699567733,-60.71925378351858,11.415870428145487,6.909184861421227,-458.3538426710045,55.76117817031178,-974.1813069950847,-1091.0782784402631,-786.5494850218674,-532.2731545342187,-496.0303093061545,-345.7712444220496,1.5838435371012554,0.10774502014756127,-0.49853538927677676,-0.002522100067449775,-3.625199287231524e-05,0.00014577518437914764,-0.3622031224473479,-0.20571464583199664,1.681723410510256,0.026396472309333237,0.005741649021498905,-0.006086370791932426,390.4635878713209,-154.22191110615756,3.65864546727504,-166.26589186227352,-125.22639802025408,367.3559392232358,0.0,130.9132132095068,512.1784170623597,-191.39283218286795,-702.1328270443303,195.00717131303867,108.27692456256575,2522.7322135369745,649.6759497990539,-12156.192251217139,419.0892342645898,1085.7775613073252,1154.1228229931921,-1518.6776203651264,-1520.957781235902,172.7765956519319,-1647.4212607274737,163.74799257770115,-409.74981617930615,-84.32677193066796,192.30007843448493,646.6678793863939,618.582966951881]\nIntercept: -2238.700528634669\nRMSE: 11489.667891\nr2: 0.023451\nnumIterations: 101\nobjectiveHistory: [0.49999999999999994, 0.4924783795518536, 0.4908814386868671, 0.4901166474101034, 0.4895534932496153, 0.4894341887116669, 0.48921657376751015, 0.48910197328378363, 0.4889059057108301, 0.48865739555158233, 0.4886128653796826, 0.48856217006273706, 0.4885029207742544, 0.48846757012170317, 0.4884521365664961, 0.4884236845911598, 0.4884101271510762, 0.4883898743082184, 0.48838989432748936, 0.4883725227372676, 0.48836720417302587, 0.48836111583530795, 0.48835291740780584, 0.48834800254567584, 0.48834535968058884, 0.48833698909003, 0.48833354397034456, 0.48832796515926463, 0.4883190631065796, 0.48831812059414864, 0.48831385278888056, 0.488313332511909, 0.4883131625488175, 0.4883116786726491, 0.4883111600097397, 0.4883101994850278, 0.48830973428211866, 0.4883090378452079, 0.48830858220832835, 0.4883083181583349, 0.4883078305689659, 0.48830761121180954, 0.4883073270503351, 0.48830713443692614, 0.4883069849560449, 0.48830682044380425, 0.4883066519158354, 0.4883065442738554, 0.4883060402554639, 0.48830586152206457, 0.48830541565311847, 0.48830508670073947, 0.48830473986522493, 0.4883044290153035, 0.4883039617605335, 0.48830355637779194, 0.4883029840683983, 0.48830236454730547, 0.4883015786310858, 0.48830134473005843, 0.4883011885490833, 0.48830070642188766, 0.48830066992535603, 0.4883006590324904, 0.48830065124397615, 0.4883006486486764, 0.48830061601931124, 0.48830061045719714, 0.4883005881624127, 0.48830055853617127, 0.48830051290671783, 0.4883003815129552, 0.48830032188284855, 0.48830018132864184, 0.4883000968703507, 0.4882999589477144, 0.4882998594004989, 0.4882997425974408, 0.48829965172956064, 0.48829958647600463, 0.4882995263848567, 0.4882994809563127, 0.48829943068025045, 0.48829939882427675, 0.4882993532802947, 0.4882993300570926, 0.4882992896513812, 0.48829926674231844, 0.48829922999948033, 0.4882992205346502, 0.48829917396304, 0.48829916516517674, 0.48829912004420334, 0.4882991084312399, 0.4882990674570376, 0.48829905713644467, 0.48829901555431815, 0.48829900629947487, 0.48829896629024705, 0.4882989441156249, 0.4882989247627871]\n"}], "source": "# Your answer goes here\n"}, {"cell_type": "code", "execution_count": 25, "metadata": {}, "outputs": [], "source": "# SOLUTION: Uncomment and execute the cell below to get help\n#disp(hint + '12-01-summ')"}, {"cell_type": "markdown", "metadata": {}, "source": "* Try again but this time divid the data into train and test and assess the model on the unseen data\n* Design a pipeline and perform hyper-parameter tunning"}, {"cell_type": "markdown", "metadata": {}, "source": "## Decision Trees\n\nDecision trees as applied to regression work fairly similarly to decision trees applied to classification. The main difference is that decision trees for regression output a single number per leaf node instead of a label (as we saw with classification). The same interpretability properties and model structure still apply. In short, rather than trying to train coeffiecients to model a function, decision tree regression simply creates a tree to predict the numerical outputs. This is of significant consequence because unlike generalized linear regression, we can predict nonlinear functions in the input data. This also creates a significant risk of overfitting the data, so we need to be careful when tuning and evaluating these models.\n\nWe also covered decision trees in the previous class. For more information on this topic, consult [An Introduction to Statistical Learning](http://www-bcf.usc.edu/~gareth/ISL/).\n\n### Model Hyperparameters\n\nThe model hyperparameters that apply decision trees for regression are the same as those for classification except for a slight change to the impurity parameter. See notebook from previous class for more information on the other hyperparameters:\n\n**impurity**\n\nThe impurity parameter represents the metric (information gain) for whether or not the model should split at a particular leaf node with a particular value or keep it as is. The only metric currently supported for regression trees is \u201cvariance.\u201d\n\n### Training Parameters\n\nIn addition to hyperparameters, classification and regression trees also share the same training parameters. \n\n**Example**\n\nHere\u2019s a short example of using a decision tree regressor:"}, {"cell_type": "code", "execution_count": 26, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "cacheNodeIds: If false, the algorithm will pass trees to executors to match instances with nodes. If true, the algorithm will cache node IDs for each instance. Caching can speed up training of deeper trees. Users can set how often should the cache be checkpointed or disable it by setting checkpointInterval. (default: False)\ncheckpointInterval: set checkpoint interval (>= 1) or disable checkpoint (-1). E.g. 10 means that the cache will get checkpointed every 10 iterations. Note: this setting will be ignored if the checkpoint directory is not set in the SparkContext. (default: 10)\nfeaturesCol: features column name. (default: features)\nimpurity: Criterion used for information gain calculation (case-insensitive). Supported options: variance (default: variance)\nlabelCol: label column name. (default: label)\nmaxBins: Max number of bins for discretizing continuous features.  Must be >=2 and >= number of categories for any categorical feature. (default: 32)\nmaxDepth: Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. (default: 5)\nmaxMemoryInMB: Maximum memory in MB allocated to histogram aggregation. If too small, then 1 node will be split per iteration, and its aggregates may exceed this size. (default: 256)\nminInfoGain: Minimum information gain for a split to be considered at a tree node. (default: 0.0)\nminInstancesPerNode: Minimum number of instances each child must have after split. If a split causes the left or right child to have fewer than minInstancesPerNode, the split will be discarded as invalid. Should be >= 1. (default: 1)\npredictionCol: prediction column name. (default: prediction)\nseed: random seed. (default: -1407754390808368278)\nvarianceCol: column name for the biased sample variance of prediction. (undefined)\n"}], "source": "from pyspark.ml.regression import DecisionTreeRegressor\ndtr = DecisionTreeRegressor()\nprint(dtr.explainParams())\ndtrModel = dtr.fit(small_df)"}, {"cell_type": "code", "execution_count": 27, "metadata": {}, "outputs": [], "source": "from pyspark.ml import Pipeline\nfrom pyspark.ml.regression import DecisionTreeRegressor\nfrom pyspark.ml.feature import VectorIndexer\n\n# Split the data into training and test sets (30% held out for testing)\n(trainingData, testData) = small_df.randomSplit([0.8, 0.2])\n\n# Train a DecisionTree model.\ndt = DecisionTreeRegressor(featuresCol=\"features\")\n\n# Train model.  This also runs the indexer.\nmodel = dt.fit(trainingData)"}, {"cell_type": "markdown", "metadata": {}, "source": "Predicting and evaluating on test set:"}, {"cell_type": "code", "execution_count": 28, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+----------+-----+--------------+\n|prediction|label|      features|\n+----------+-----+--------------+\n|       0.0|  0.0|[1.0,0.1,-1.0]|\n|       2.0|  1.0| [2.0,1.1,1.0]|\n|       2.0|  2.0|[3.0,10.1,3.0]|\n+----------+-----+--------------+\n\n"}], "source": "# Make predictions.\npredictions = model.transform(testData)\n\n# Select example rows to display.\npredictions.select(\"prediction\", \"label\", \"features\").show(5)"}, {"cell_type": "code", "execution_count": 29, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Root Mean Squared Error (RMSE) on test data = 0.57735\nDecisionTreeRegressionModel (uid=DecisionTreeRegressor_4faf86cc1e9de61be821) of depth 1 with 3 nodes\n"}], "source": "from pyspark.ml.evaluation import RegressionEvaluator\n\n# Select (prediction, true label) and compute test error\nevaluator = RegressionEvaluator(metricName=\"rmse\")\n\nrmse = evaluator.evaluate(predictions)\n\nprint(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)\n\n# Summary\nprint(model)"}, {"cell_type": "markdown", "metadata": {}, "source": "# Your turn\n\nRepeat this with the Online News Popularity Data Set:"}, {"cell_type": "code", "execution_count": 30, "metadata": {}, "outputs": [], "source": "# Your answer goes here\n"}, {"cell_type": "markdown", "metadata": {}, "source": "Make predictions on test set and check the model performance against it:"}, {"cell_type": "code", "execution_count": 31, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+------------------+------+--------------------+\n|        prediction| label|            features|\n+------------------+------+--------------------+\n|2260.3359324236517| 556.0|(59,[0,1,2,3,4,5,...|\n|3246.7258485639686|3600.0|(59,[0,1,2,3,4,5,...|\n|2260.3359324236517|2200.0|(59,[0,1,2,3,4,5,...|\n| 2500.480752780154| 823.0|(59,[0,1,2,3,4,5,...|\n|2260.3359324236517|3100.0|(59,[0,1,2,3,4,5,...|\n+------------------+------+--------------------+\nonly showing top 5 rows\n\n"}], "source": "# Your answer goes here\n"}, {"cell_type": "code", "execution_count": 32, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Root Mean Squared Error (RMSE) on test data = 16526.3\nDecisionTreeRegressionModel (uid=DecisionTreeRegressor_4637b1bdd067400442b4) of depth 5 with 63 nodes\n"}], "source": "# Your answer goes here\n"}, {"cell_type": "markdown", "metadata": {}, "source": "* What kind of improvements can you make to better this model?"}], "metadata": {"kernelspec": {"display_name": "PySpark", "language": "python", "name": "pyspark"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.8.6"}}, "nbformat": 4, "nbformat_minor": 4}