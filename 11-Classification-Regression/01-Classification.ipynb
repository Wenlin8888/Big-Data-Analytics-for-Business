{"cells": [{"cell_type": "markdown", "metadata": {}, "source": "# Classification\n\nClassification is the task of predicting a label, category, class, or discrete variable given some input features. The key difference from other ML tasks, such as regression, is that the output label has a finite set of possible values (e.g., three classes).\n\nHere are a few of the multitude of ways classification can be used in the real world.\n\n**Predicting credit risk**\n\nA financing company might look at a number of variables before offering a loan to a company or individual. Whether or not to offer the loan is a binary classification problem.\n\n**News classification**\n\nAn algorithm might be trained to predict the topic of a news article (sports, politics, business, etc.).\n\n**Classifying human activity**\n\nBy collecting data from sensors such as a phone accelerometer or smart watch, you can predict the person\u2019s activity. The output will be one of a finite set of classes (e.g., walking, sleeping, standing, or running).\n\n## Types of Classification\n\nBefore we continue, let\u2019s review several different types of classification.\n\n### Binary Classification\n\nThe simplest example of classification is binary classification, where there are only two labels you can predict. One example is fraud analytics, where a given transaction can be classified as fraudulent or not; or email spam, where a given email can be classified as spam or not spam.\n\n### Multiclass Classification\n\nBeyond binary classification lies multiclass classification, where one label is chosen from more than two distinct possible labels. A typical example is Facebook predicting the people in a given photo or a meterologist predicting the weather (rainy, sunny, cloudy, etc.). Note how there is always a finite set of classes to predict; it\u2019s never unbounded. This is also called multinomial classification.\n\n### Multilabel Classification\n\nFinally, there is multilabel classification, where a given input can produce multiple labels. For example, you might want to predict a book\u2019s genre based on the text of the book itself. While this could be multiclass, it\u2019s probably better suited for multilabel because a book may fall into multiple genres. Another example of multilabel classification is identifying the number of objects that appear in an image. Note that in this example, the number of output predictions is not necessarily fixed, and could vary from image to image."}, {"cell_type": "markdown", "metadata": {}, "source": "## Classification Models in MLlib\n\nSpark has several models available for performing binary and multiclass classification out of the box. The following models are available for classification in Spark:\n\n* [Logistic regression](http://spark.apache.org/docs/latest/ml-classification-regression.html#logistic-regression)\n* [Decision trees](http://spark.apache.org/docs/latest/ml-classification-regression.html#decision-tree-classifier)\n* [Random forests](http://spark.apache.org/docs/latest/ml-classification-regression.html#random-forest-classifier)\n* [Gradient-boosted trees](http://spark.apache.org/docs/latest/ml-classification-regression.html#gradient-boosted-tree-classifier)\n* [Multilayer perceptron classifier](http://spark.apache.org/docs/latest/ml-classification-regression.html#multilayer-perceptron-classifier)\n\nFor a full list of all available classifiers in Spark checkout the [documentation](http://spark.apache.org/docs/latest/ml-classification-regression.html#classification).\n\nSpark does not support making multilabel predictions natively. In order to train a multilabel model, you must train one model per label and combine them manually. Once manually constructed, there are built-in tools that support measuring these kinds of models.\n\nThis notebook will cover the basics of each of these models by providing:\n\n* A simple explanation of the model and the intuition behind it\n* Model hyperparameters (the different ways we can initialize the model)\n* Training parameters (parameters that affect how the model is trained)\n* Prediction parameters (parameters that affect how predictions are made)\n\nYou can set the hyperparameters and training parameters in a `ParamGrid`."}, {"cell_type": "markdown", "metadata": {}, "source": "### Model Scalability\n\nModel scalability is an important consideration when choosing your model. In general, Spark has great support for training large-scale machine learning models (note, these are large scale; on single-node workloads there are a number of other tools that also perform well). Table below is a simple model scalability scorecard to use to find the best model for your particular task (if scalability is your core consideration). The actual scalability will depend on your configuration, machine size, and other specifics but should make for a good heuristic.\n\nModel scalability reference\n\n|Model|\tFeatures count|\tTraining examples|\tOutput classes|\n|--|--|--|--|\n|Logistic regression|1 to 10 million|No limit|Features x Classes < 10 million|\n|Decision trees|1,000s|No limit|Features x Classes < 10,000s|\n|Random forest|10,000s|No limit|Features x Classes < 100,000s|\n|Gradient-boosted trees|1,000s|No limit|Features x Classes < 10,000s|"}, {"cell_type": "markdown", "metadata": {}, "source": "We can see that nearly all these models scale to large collections of input data and there is ongoing work to scale them even further. The reason no limit is in place for the number of training examples is because these are trained using methods like stochastic gradient descent. These methods are optimized specifically for working with massive datasets and to remove any constraints that might exist on the number of training examples you would hope to learn on."}, {"cell_type": "markdown", "metadata": {}, "source": "## Logistic Regression\n\nLogistic regression is one of the most popular methods of classification. It is a linear method that combines each of the individual inputs (or features) with specific weights (these weights are generated during the training process) that are then combined to get a probability of belonging to a particular class. These weights are helpful because they are good representations of feature importance; if you have a large weight, you can assume that variations in that feature have a significant effect on the outcome (assuming you performed normalization). A smaller weight means the feature is less likely to be important.\n\nConsider the following example: The *Default* Dataset"}, {"cell_type": "markdown", "metadata": {}, "source": "<img src=\"https://github.com/soltaniehha/Big-Data-Analytics-for-Business/blob/master/figs/11-01-the-default-dataset.png?raw=true\" width=\"800\" align=\"center\"/>\n\n<div style=\"text-align: center\"> The Default data set. Left: The annual incomes and monthly credit card balances of a number of individuals. The individuals who defaulted on their credit card payments are shown in orange, and those who did not are shown in blue. Center: Boxplots of balance as a function of default status. Right: Boxplots of income as a function of default status. </div>\n\n*<div style=\"text-align: right\"> From [An Introduction to Statistical Learning](http://www-bcf.usc.edu/~gareth/ISL/) by Gareth James, et al </div>*"}, {"cell_type": "markdown", "metadata": {}, "source": "Considering this data where the response \"default\" falls into a \"Yes\" or \"No\" category, let's find a model that can predict the probability of default by using the \"Balance\" on the credit card. Rather than modeling this response Y\ndirectly, logistic regression models the probability that Y belongs to a particular category.\n\nThe probability of default given balance can be written as:\n> **Pr(**default = Yes|balance**)**\n\nThe values of Pr(default = Yes|balance), which we abbreviate p(balance), will range between 0 and 1. Then for any given value of balance, a prediction can be made for default. For example, one might predict default = Yes for any individual for whom p(balance) > 0.5. Alternatively, if a company wishes to be conservative in predicting individuals who are at risk for default, then they may choose to use a lower threshold, such as p(balance) > 0.1."}, {"cell_type": "markdown", "metadata": {}, "source": "\n<img src=\"https://github.com/soltaniehha/Big-Data-Analytics-for-Business/blob/master/figs/11-01-Classification-lr.png?raw=true\" width=\"800\" align=\"center\"/>\n\n<div style=\"text-align: center\"> Classification using the Default data. Left: Estimated probability of default using linear regression. Some estimated probabilities are negative! The orange ticks indicate the 0/1 values coded for default(No or Yes). Right: Predicted probabilities of default using logistic regression. All probabilities lie between 0 and 1.\n </div>\n\n*<div style=\"text-align: right\"> From [An Introduction to Statistical Learning](http://www-bcf.usc.edu/~gareth/ISL/) by Gareth James, et al </div>*"}, {"cell_type": "markdown", "metadata": {}, "source": "### The Logistic Model\n\nHow should we model the relationship between $p(X) = Pr(Y = 1|X)$ and $X$? (For convenience we are using the generic 0/1 coding for the response). One approach would be using a linear regression model to represent these probabilities:\n\n> $p(X) = \\beta_0 + \\beta_1 X$\n\nIf we use this approach to predict default=Yes using balance, then we obtain the model shown in the left-hand panel of figure above. Here we see the problem with this approach: for balances close to zero we predict a negative probability of default; if we were to predict for very large balances, we would get values bigger than 1. These predictions are not sensible, since of course the true probability of default, regardless of credit card balance, must fall between 0 and 1.\n\nTo avoid this problem, we must model $p(X)$ using a function that gives outputs between 0 and 1 for all values of $X$. Many functions meet this description. In logistic regression, we use the logistic function:\n\n> $p(X) = \\dfrac{e ^ {\\beta_0 + \\beta_1 X }}{1 + e ^ {\\beta_0 + \\beta_1 X }}$"}, {"cell_type": "markdown", "metadata": {}, "source": "To fit the model above, we use a method called `maximum likelihood`. The right-hand panel of figure above illustrates the fit of the logistic regression model to the Default data. Notice that for low balances we now predict the probability of default as close to, but never below, zero. Likewise, for high balances we predict a default probability close to, but never above, one. The logistic function will always produce an S-shaped curve of this form, and so regardless of the value of X, we will obtain a sensible prediction. We also see that the logistic model is\nbetter able to capture the range of probabilities than is the linear regression model in the left-hand plot. \n\nAfter a bit of manipulation of the logistic function, we find that:\n\n> $\\dfrac{p(X)}{1-p(X)} = e ^ {\\beta_0 + \\beta_1 X }$\n\nThe quantity $p(X)/[1\u2212p(X)]$ is called the odds, and can take on any value between $0$ and $\\infty$. Values of the odds close to $0$ and $\\infty$ indicate very low and very high probabilities of default, respectively. \n\nFor example, Let\u2019s say that the probability of default is $0.2$ (or $1$ in $5$ people), this will give an odds of default equal to $1$ to $4$. This means that the odds of someone defaulting is $25\\%$ of not defaulting. Since $p(X) = 0.2$ implies an odds of $\\dfrac{0.2}{1-0.2} = 1/4$. \n\nLikewise on average $9$ out of every $10$ people with an odds of $9$ will default, since $p(X) = 0.9$ implies an odds of $\\dfrac{0.9}{1-0.9} = 9$.\n\nOdds are traditionally used instead of probabilities in horse-racing, since they relate more naturally to the correct betting strategy.\n\nBy taking the logarithm of both sides of the equation above, we arrive at\n\n> $log(\\dfrac{p(X)}{1-p(X)}) = \\beta_0 + \\beta_1 X$\n"}, {"cell_type": "markdown", "metadata": {}, "source": "The left-hand side is called the *log-odds* or *logit*. We see that the logistic regression mdoel (defined above) has a logit that is linear in $X$.\n\nThe table below shows estimated coefficients of the logistic regression model that predicts the probability of \"default\" using \"balance\":\n\n| - |Coefficient| Std. error| Z-statistic| P-value|\n|--|--|--|--|--|\n|Intercept| \u221210.6513| 0.3612| \u221229.5| <0.0001|\n|balance |0.0055 |0.0002 |24.9 |<0.0001|"}, {"cell_type": "markdown", "metadata": {}, "source": "We can measure the accuracy of the coefficient estimates by computing their standard errors. The z-statistic in table above plays the same role as the t-statistic in the linear regression output. A large (absolute) value of the z-statistic indicates evidence against the null hypothesis $H0 : \u03b21 = 0$. This null hypothesis implies that $p(X) = \\dfrac{e ^ {\\beta_0 }}{1 + e ^ {\\beta_0 }}$. In other words, that the probability of default does not depend on balance. Since the p-value associated with balance in the table is tiny, we can reject $H0$. In other words, we conclude that there is indeed an association between balance and probability of default. \n\nThe estimated intercept in the table is typically not of interest; its main purpose is to adjust the average fitted\nprobabilities to the proportion of ones in the data."}, {"cell_type": "markdown", "metadata": {}, "source": "### Model Hyperparameters\n\nModel hyperparameters are configurations that determine the basic structure of the model itself. The following hyperparameters are available for logistic regression:\n\n**`family`** (default: auto)\n\nCan be multinomial (two or more distinct labels; multiclass classification) or binary (only two distinct labels; binary classification).\n\n**`elasticNetParam`** (default: 0.0)\n\nA floating-point value from 0 to 1. This parameter specifies the mix of L1 and L2 regularization according to elastic net regularization (which is a linear combination of the two). Your choice of L1 or L2 depends a lot on your particular use case but the intuition is as follows: L1 regularization (a value of 1) will create sparsity in the model because certain feature weights will become zero (that are of little consequence to the output). For this reason, it can be used as a simple feature-selection method. On the other hand, L2 regularization (a value of 0) does not create sparsity because the corresponding weights for particular features will only be driven toward zero, but will never completely reach zero. ElasticNet gives us the best of both worlds\u2014we can choose a value between 0 and 1 to specify a mix of L1 and L2 regularization. For the most part, you should be tuning this by testing different values.\n\nNote: Regularization is a technique to discourage the complexity of the model. It does this by penalizing the loss function. This helps to solve the overfitting problem. Here is a brief [Medium](https://medium.com/datadriveninvestor/l1-l2-regularization-7f1b4fe948f2) article to go over this concept.\n\n**`fitIntercept`** (default: True)\n\nCan be true or false. This hyperparameter determines whether or not to fit the intercept or the arbitrary number that is added to the linear combination of inputs and weights of the model. Typically you will want to fit the intercept if we haven\u2019t normalized our training data.\n\n**`regParam`** (default: 0.0)\n\nA value \u2265 0. that determines how much weight to give to the regularization term in the objective function. Choosing a value here is again going to be a function of noise and dimensionality in our dataset. In a pipeline, try a wide range of values (e.g., 0, 0.01, 0.1, 1).\n\n**`standardization`** (default: True)\n\nCan be true or false, whether or not to standardize the inputs before passing them into the model."}, {"cell_type": "markdown", "metadata": {}, "source": "### Training Parameters\n\nTraining parameters are used to specify how we perform our training. Here are the training parameters for logistic regression:\n\n**`maxIter`** (default: 100)\n\nTotal number of iterations over the data before stopping. Changing this parameter probably won\u2019t change your results a ton, so it shouldn\u2019t be the first parameter you look to adjust.\n\n**`tol`** (default: 1e-06)\n\nThis convergence tolerance specifies a threshold by which changes in parameters show that we optimized our weights enough, and can stop iterating. It lets the algorithm stop before `maxIter` iterations. The default value is 1.0E-6. This also shouldn\u2019t be the first parameter you look to tune.\n\n**`weightCol`** (undefined)\n\nThe name of a weight column used to weigh certain rows more than others. This can be a useful tool if you have some other measure of how important a particular training example is and have a weight associated with it. For example, you might have 10,000 examples where you know that some labels are more accurate than others. You can weigh the labels you know are correct more than the ones you don\u2019t. If this is not set or empty, we treat all instance weights as 1.0."}, {"cell_type": "markdown", "metadata": {}, "source": "### Prediction Parameters\n\nThese parameters help determine how the model should actually be making predictions at prediction time, but do not affect training. Here are the prediction parameters for logistic regression:\n\n**`threshold`** (default: 0.5)\n\nThreshold in binary classification prediction, in range [0, 1]. This parameter is the probability threshold for when a given class should be predicted. You can tune this parameter according to your requirements to balance between false positives and false negatives. For instance, if a mistaken prediction would be costly\u2014you might want to make its prediction threshold very high.\n\n**`thresholds`** (undefined)\n\nThis parameter lets you specify an array of threshold values for each class when using multiclass classification. It works similarly to the single threshold parameter described previously."}, {"cell_type": "markdown", "metadata": {}, "source": "## Example\n\nLet\u2019s start looking at the classification models by loading in some data:"}, {"cell_type": "code", "execution_count": 1, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "gs://is843/notebooks/jupyter/data/\n"}], "source": "# the following line gets the bucket name attached to our cluster\nbucket = spark._jsc.hadoopConfiguration().get(\"fs.gs.system.bucket\")\n\n# specifying the path to our bucket where the data is located (no need to edit this path anymore)\ndata = \"gs://\" + bucket + \"/notebooks/jupyter/data/\"\nprint(data)"}, {"cell_type": "code", "execution_count": 2, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "sales datasets consists of 5 rows.\n+--------------+-----+\n|      features|label|\n+--------------+-----+\n|[3.0,10.1,3.0]|  1.0|\n|[1.0,0.1,-1.0]|  0.0|\n|[1.0,0.1,-1.0]|  0.0|\n| [2.0,1.1,1.0]|  1.0|\n| [2.0,1.1,1.0]|  1.0|\n+--------------+-----+\n\n"}], "source": "bInput = spark.read.format(\"parquet\").load(data + \"binary-classification\")\\\n  .selectExpr(\"features\", \"cast(label as double) as label\")\n\nbInput.cache()\nprint(\"sales datasets consists of {} rows.\".format(bInput.count()))\nbInput.show()"}, {"cell_type": "markdown", "metadata": {}, "source": "Here\u2019s a simple example using the `LogisticRegression` model. Notice how we didn\u2019t specify any parameters because we\u2019ll leverage the defaults and our data conforms to the proper column naming. In practice, you probably won\u2019t need to change many of the parameters:"}, {"cell_type": "code", "execution_count": 3, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "aggregationDepth: suggested depth for treeAggregate (>= 2). (default: 2)\nelasticNetParam: the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty. (default: 0.0)\nfamily: The name of family which is a description of the label distribution to be used in the model. Supported options: auto, binomial, multinomial (default: auto)\nfeaturesCol: features column name. (default: features)\nfitIntercept: whether to fit an intercept term. (default: True)\nlabelCol: label column name. (default: label)\nlowerBoundsOnCoefficients: The lower bounds on coefficients if fitting under bound constrained optimization. The bound matrix must be compatible with the shape (1, number of features) for binomial regression, or (number of classes, number of features) for multinomial regression. (undefined)\nlowerBoundsOnIntercepts: The lower bounds on intercepts if fitting under bound constrained optimization. The bounds vector size must beequal with 1 for binomial regression, or the number oflasses for multinomial regression. (undefined)\nmaxIter: max number of iterations (>= 0). (default: 100)\npredictionCol: prediction column name. (default: prediction)\nprobabilityCol: Column name for predicted class conditional probabilities. Note: Not all models output well-calibrated probability estimates! These probabilities should be treated as confidences, not precise probabilities. (default: probability)\nrawPredictionCol: raw prediction (a.k.a. confidence) column name. (default: rawPrediction)\nregParam: regularization parameter (>= 0). (default: 0.0)\nstandardization: whether to standardize the training features before fitting the model. (default: True)\nthreshold: Threshold in binary classification prediction, in range [0, 1]. If threshold and thresholds are both set, they must match.e.g. if threshold is p, then thresholds must be equal to [1-p, p]. (default: 0.5)\nthresholds: Thresholds in multi-class classification to adjust the probability of predicting each class. Array must have length equal to the number of classes, with values > 0, excepting that at most one value may be 0. The class with largest value p/t is predicted, where p is the original probability of that class and t is the class's threshold. (undefined)\ntol: the convergence tolerance for iterative algorithms (>= 0). (default: 1e-06)\nupperBoundsOnCoefficients: The upper bounds on coefficients if fitting under bound constrained optimization. The bound matrix must be compatible with the shape (1, number of features) for binomial regression, or (number of classes, number of features) for multinomial regression. (undefined)\nupperBoundsOnIntercepts: The upper bounds on intercepts if fitting under bound constrained optimization. The bound vector size must be equal with 1 for binomial regression, or the number of classes for multinomial regression. (undefined)\nweightCol: weight column name. If this is not set or empty, we treat all instance weights as 1.0. (undefined)\n"}], "source": "from pyspark.ml.classification import LogisticRegression\n\nlr = LogisticRegression()\nprint(lr.explainParams())  # see all parameters"}, {"cell_type": "code", "execution_count": 4, "metadata": {}, "outputs": [], "source": "lrModel = lr.fit(bInput)"}, {"cell_type": "markdown", "metadata": {}, "source": "Once the model is trained you can get information about the model by taking a look at the coefficients and the intercept. The coefficients correspond to the individual feature weights (each feature weight is multiplied by each respective feature to compute the prediction) while the intercept is the value of the italics-intercept (if we chose to fit one when specifying the model). Seeing the coefficients can be helpful for inspecting the model that you built and comparing how features affect the prediction:"}, {"cell_type": "code", "execution_count": 5, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "[6.848741326854929,0.3535658901019745,14.814900276915923]\n-10.22569586448109\n"}], "source": "print(lrModel.coefficients)\nprint(lrModel.intercept)"}, {"cell_type": "markdown", "metadata": {}, "source": "For a multinomial model (the current one is binary), `lrModel.coefficientMatrix` and `lrModel.interceptVector` can be used to get the coefficients and intercept. These will return Matrix and Vector types representing the values or each of the given classes.\n\n### Model Summary\n\nLogistic regression provides a model summary that gives you information about the final, trained model. This is analogous to the same types of summaries we see in many R language machine learning packages. The model summary is currently only available for binary logistic regression problems, but multiclass summaries will likely be added in the future. \n\nUsing the binary summary, we can get all sorts of information about the model itself including the area under the ROC curve, the f measure by threshold, the precision, the recall, the recall by thresholds, and the ROC curve. You can see the summary using the following APIs:"}, {"cell_type": "code", "execution_count": 6, "metadata": {}, "outputs": [], "source": "summary = lrModel.summary"}, {"cell_type": "markdown", "metadata": {}, "source": "**Area under the ROC curve**\n\nAUC - ROC curve is a performance measurement for classification problem at various thresholds settings. ROC is a probability curve and AUC represents degree or measure of separability. It tells how much model is capable of distinguishing between classes. Higher the AUC, better the model is at predicting 0s as 0s and 1s as 1s. By analogy, Higher the AUC, better the model is at distinguishing between patients with disease and no disease. AUC typically varies between 0.5 and 1. A 0.5 AUC is indicating that the model is as good as a random guess, 1 is a perfect model."}, {"cell_type": "code", "execution_count": 7, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "AUC: 1.0\n"}], "source": "print(\"AUC:\", summary.areaUnderROC)"}, {"cell_type": "markdown", "metadata": {}, "source": "ROC curve:"}, {"cell_type": "code", "execution_count": 8, "metadata": {}, "outputs": [{"data": {"text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>FPR</th>\n      <th>TPR</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.0</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.0</td>\n      <td>0.333333</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.0</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1.0</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1.0</td>\n      <td>1.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>", "text/plain": "   FPR       TPR\n0  0.0  0.000000\n1  0.0  0.333333\n2  0.0  1.000000\n3  1.0  1.000000\n4  1.0  1.000000"}, "execution_count": 8, "metadata": {}, "output_type": "execute_result"}], "source": "roc = summary.roc.toPandas()\nroc"}, {"cell_type": "markdown", "metadata": {}, "source": "The ROC curve is plotted with TPR against the FPR where TPR is on y-axis and FPR is on the x-axis:"}, {"cell_type": "code", "execution_count": 9, "metadata": {}, "outputs": [{"data": {"text/plain": "Text(0,0.5,'TPR')"}, "execution_count": 9, "metadata": {}, "output_type": "execute_result"}, {"data": {"image/png": "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\n", "text/plain": "<Figure size 432x288 with 1 Axes>"}, "metadata": {"needs_background": "light"}, "output_type": "display_data"}], "source": "%matplotlib inline\nimport matplotlib.pyplot as plt\n\nroc.plot(x='FPR', y='TPR', xlim = [-0.1, 1], style='--o', legend=False)\nplt.title('ROC Cruve')\nplt.ylabel('TPR')"}, {"cell_type": "markdown", "metadata": {}, "source": "Precision-Recall is a useful measure of success of prediction when the classes are very imbalanced."}, {"cell_type": "code", "execution_count": 10, "metadata": {}, "outputs": [{"data": {"text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>recall</th>\n      <th>precision</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.000000</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.333333</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1.000000</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1.000000</td>\n      <td>0.6</td>\n    </tr>\n  </tbody>\n</table>\n</div>", "text/plain": "     recall  precision\n0  0.000000        1.0\n1  0.333333        1.0\n2  1.000000        1.0\n3  1.000000        0.6"}, "execution_count": 10, "metadata": {}, "output_type": "execute_result"}], "source": "pr = summary.pr.toPandas()\npr"}, {"cell_type": "code", "execution_count": 11, "metadata": {}, "outputs": [{"data": {"text/plain": "Text(0,0.5,'precision')"}, "execution_count": 11, "metadata": {}, "output_type": "execute_result"}, {"data": {"image/png": "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\n", "text/plain": "<Figure size 432x288 with 1 Axes>"}, "metadata": {"needs_background": "light"}, "output_type": "display_data"}], "source": "pr.plot(x='recall', y='precision', style='--o', legend=False)\nplt.title('Precision-Recall Cruve')\nplt.ylabel('precision')"}, {"cell_type": "markdown", "metadata": {}, "source": "The speed at which the model descends to the final result is shown in the objective history. We can access this through the objective history on the model summary:"}, {"cell_type": "code", "execution_count": 12, "metadata": {}, "outputs": [{"data": {"text/plain": "[0.6730116670092563,\n 0.5042829330409727,\n 0.36356862066874396,\n 0.1252407018038337,\n 0.08532556611276214,\n 0.035504876415730455,\n 0.018196494508571255,\n 0.008817369922959136,\n 0.004413673785392143,\n 0.002194038351234709,\n 0.0010965641148080857,\n 0.000547657551985314,\n 0.00027376237951490126,\n 0.0001368465223657475,\n 6.841809037070595e-05,\n 3.420707791038497e-05,\n 1.7103176664232043e-05,\n 8.551470106426904e-06,\n 4.275703677941461e-06,\n 2.1378240117781303e-06,\n 1.068856405465203e-06,\n 5.34260020257524e-07,\n 2.668135105897439e-07,\n 1.3204627865316843e-07,\n 6.768401481686428e-08,\n 3.314547718487037e-08,\n 1.6151438837494788e-08,\n 8.309350118269286e-09]"}, "execution_count": 12, "metadata": {}, "output_type": "execute_result"}], "source": "summary.objectiveHistory"}, {"cell_type": "markdown", "metadata": {}, "source": "## Decision Trees\n\nDecision trees are one of the more friendly and interpretable models for performing classification because they\u2019re similar to simple decision models that humans use quite often. For example, if you have to predict whether or not someone will eat ice cream when offered, a good feature might be whether or not that individual likes ice cream. In pseudocode, if person.likes(\u201cice_cream\u201d), they will eat ice cream; otherwise, they won\u2019t eat ice cream. A decision tree creates this type of structure with all the inputs and follows a set of branches when it comes time to make a prediction. This makes it a great starting point model because it\u2019s easy to reason about, easy to inspect, and makes very few assumptions about the structure of the data. In short, rather than trying to train coeffiecients in order to model a function, it simply creates a big tree of decisions to follow at prediction time. This model also supports multiclass classification and provides outputs as predictions and probabilities in two different columns.\n\n\n<img src=\"https://github.com/soltaniehha/Big-Data-Analytics-for-Business/blob/master/figs/11-01-Decision-Tree.png?raw=true\" width=\"800\" align=\"center\"/>\n\n*<div style=\"text-align: right\"> From [An Introduction to Statistical Learning](http://www-bcf.usc.edu/~gareth/ISL/) by Gareth James, et al </div>*\n\nWhile this model is usually a great start, it does come at a cost. It can overfit data extremely quickly. By that we mean that, unrestrained, the decision tree will create a pathway from the start based on every single training example. That means it encodes all of the information in the training set in the model. This is bad because then the model won\u2019t generalize to new data (you will see poor test set prediction performance). However, there are a number of ways to try and rein in the model by limiting its branching structure (e.g., limiting its height) to get good predictive power."}, {"cell_type": "markdown", "metadata": {}, "source": "### Model Hyperparameters\n\nThere are many different ways to configure and train decision trees. Here are the hyperparameters that Spark\u2019s implementation supports:\n\n**`maxDepth`** (default: 5)\n\nSince we\u2019re training a tree, it can be helpful to specify a max depth in order to avoid overfitting to the dataset (in the extreme, every row ends up as its own leaf node). \n\n**`maxBins`** (default: 32)\n\nIn decision trees, continuous features are converted into categorical features and `maxBins` determines how many bins should be created from continous features. More bins gives a higher level of granularity. The value must be greater than or equal to 2 and greater than or equal to the number of categories in any categorical feature in your dataset. \n\n**`impurity`** (default: gini)\n\nTo build up a \u201ctree\u201d you need to configure when the model should branch. Impurity represents the metric (information gain) to determine whether or not the model should split at a particular leaf node. This parameter can be set to either be \u201centropy\u201d or \u201cgini\u201d (default), two commonly used impurity metrics.\n\n**`minInfoGain`** (default: 0.0)\n\nThis parameter determines the minimum information gain that can be used for a split. A higher value can prevent overfitting. This is largely something that needs to be determined from testing out different variations of the decision tree model.\n\n**`minInstancesPerNode`** (default: 1)\n\nMinimum number of instances each child must have after split. If a split causes the left or right child to have fewer than minInstancesPerNode, the split will be discarded as invalid. Should be >= 1.\n\n### Training Parameters\n\nThese are configurations we specify in order to manipulate how we perform our training. Here is the training parameter for decision trees:\n\n**`checkpointInterval`** \n\nCheckpointing is a way to save the model\u2019s work over the course of training so that if nodes in the cluster crash for some reason, you don\u2019t lose your work. A value of 10 means the model will get checkpointed every 10 iterations. Set this to -1 to turn off checkpointing. This parameter needs to be set together with a `checkpointDir` (a directory to checkpoint to) and with `useNodeIdCache=true`. Consult the Spark documentation for more information on checkpointing.\n\n### Prediction Parameters\n\nThere is only one prediction parameter for decision trees: `thresholds`. Refer to the explanation for thresholds under \u201cLogistic Regression\u201d.\n\nHere\u2019s a minimal but complete example of using a decision tree classifier:"}, {"cell_type": "code", "execution_count": 13, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "cacheNodeIds: If false, the algorithm will pass trees to executors to match instances with nodes. If true, the algorithm will cache node IDs for each instance. Caching can speed up training of deeper trees. Users can set how often should the cache be checkpointed or disable it by setting checkpointInterval. (default: False)\ncheckpointInterval: set checkpoint interval (>= 1) or disable checkpoint (-1). E.g. 10 means that the cache will get checkpointed every 10 iterations. Note: this setting will be ignored if the checkpoint directory is not set in the SparkContext. (default: 10)\nfeaturesCol: features column name. (default: features)\nimpurity: Criterion used for information gain calculation (case-insensitive). Supported options: entropy, gini (default: gini)\nlabelCol: label column name. (default: label)\nmaxBins: Max number of bins for discretizing continuous features.  Must be >=2 and >= number of categories for any categorical feature. (default: 32)\nmaxDepth: Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. (default: 5)\nmaxMemoryInMB: Maximum memory in MB allocated to histogram aggregation. If too small, then 1 node will be split per iteration, and its aggregates may exceed this size. (default: 256)\nminInfoGain: Minimum information gain for a split to be considered at a tree node. (default: 0.0)\nminInstancesPerNode: Minimum number of instances each child must have after split. If a split causes the left or right child to have fewer than minInstancesPerNode, the split will be discarded as invalid. Should be >= 1. (default: 1)\npredictionCol: prediction column name. (default: prediction)\nprobabilityCol: Column name for predicted class conditional probabilities. Note: Not all models output well-calibrated probability estimates! These probabilities should be treated as confidences, not precise probabilities. (default: probability)\nrawPredictionCol: raw prediction (a.k.a. confidence) column name. (default: rawPrediction)\nseed: random seed. (default: 956191873026065186)\n"}], "source": "from pyspark.ml.classification import DecisionTreeClassifier\n\ndt = DecisionTreeClassifier()\nprint(dt.explainParams())"}, {"cell_type": "code", "execution_count": 14, "metadata": {}, "outputs": [], "source": "dtModel = dt.fit(bInput)"}, {"cell_type": "code", "execution_count": 15, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+--------------+-----+-------------+-----------+----------+\n|      features|label|rawPrediction|probability|prediction|\n+--------------+-----+-------------+-----------+----------+\n|[3.0,10.1,3.0]|  1.0|    [0.0,3.0]|  [0.0,1.0]|       1.0|\n|[1.0,0.1,-1.0]|  0.0|    [2.0,0.0]|  [1.0,0.0]|       0.0|\n|[1.0,0.1,-1.0]|  0.0|    [2.0,0.0]|  [1.0,0.0]|       0.0|\n| [2.0,1.1,1.0]|  1.0|    [0.0,3.0]|  [0.0,1.0]|       1.0|\n| [2.0,1.1,1.0]|  1.0|    [0.0,3.0]|  [0.0,1.0]|       1.0|\n+--------------+-----+-------------+-----------+----------+\n\n"}], "source": "dtModel.transform(bInput).show()"}], "metadata": {"kernelspec": {"display_name": "PySpark", "language": "python", "name": "pyspark"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.10"}}, "nbformat": 4, "nbformat_minor": 2}