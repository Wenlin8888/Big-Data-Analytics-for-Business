{"cells": [{"cell_type": "markdown", "metadata": {}, "source": "# MLlib in Action\n\nNow that we have described some of the core pieces you can expect to come across, let\u2019s create a simple pipeline to demonstrate each of the components. We\u2019ll use a small synthetic dataset that will help illustrate our point. Let\u2019s read the data in and see a sample before talking about it further:"}, {"cell_type": "code", "execution_count": 1, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "gs://is843-demo/notebooks/jupyter/data/\n"}], "source": "# the following line gets the bucket name attached to our cluster\nbucket = spark._jsc.hadoopConfiguration().get(\"fs.gs.system.bucket\")\n\n# specifying the path to our bucket where the data is located (no need to edit this path anymore)\ndata = \"gs://\" + bucket + \"/notebooks/jupyter/data/\"\nprint(data)"}, {"cell_type": "code", "execution_count": 2, "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "+-----+----+------+------------------+\n|color| lab|value1|            value2|\n+-----+----+------+------------------+\n|  red|good|    35|14.386294994851129|\n| blue| bad|    12|14.386294994851129|\n|  red| bad|     2|14.386294994851129|\n| blue| bad|     8|14.386294994851129|\n|  red| bad|    16|14.386294994851129|\n+-----+----+------+------------------+\nonly showing top 5 rows\n\n"}], "source": "df = spark.read.json(data + \"simple-ml\")\ndf.orderBy(\"value2\").show(5)"}, {"cell_type": "markdown", "metadata": {}, "source": "This dataset consists of a categorical label with two values (good or bad), a categorical variable (color), and two numerical variables. While the data is synthetic, let\u2019s imagine that this dataset represents a company\u2019s customer health. The \u201ccolor\u201d column represents some categorical health rating made by a customer service representative. The \u201clab\u201d column represents the true customer health. The other two values are some numerical measures of activity within an application (e.g., minutes spent on site and purchases). Suppose that we want to train a classification model where we hope to predict a binary variable\u2014the label\u2014from the other values.\n\n## Feature Engineering with Transformers\n\nAs already mentioned, transformers help us manipulate our current columns in one way or another. Manipulating these columns is often in pursuit of building features (that we will input into our model). Transformers exist to either cut down the number of features, add more features, manipulate current ones, or simply to help us format our data correctly. Transformers add new columns to DataFrames.\n\nWhen we use MLlib, all inputs to machine learning algorithms (with several exceptions) in Spark must consist of type Double (for labels) and Vector[Double] (for features). The current dataset does not meet that requirement and therefore we need to transform it to the proper format.\n\nTo achieve this in our example, we are going to specify an **RFormula**. This is a declarative language for specifying machine learning transformations and is simple to use once you understand the syntax. RFormula supports a limited subset of the R operators that in practice work quite well for simple models and manipulations (we demonstrate the manual approach to this problem in next class). The basic RFormula operators are:\n\n~\nSeparate target and terms\n\n+\nConcat terms; \u201c+ 0\u201d means removing the intercept (this means that the y-intercept of the line that we will fit will be 0)\n\n-\nRemove a term; \u201c- 1\u201d means removing the intercept (this means that the y-intercept of the line that we will fit will be 0\u2014yes, this does the same thing as \u201c+ 0\u201d\n\n:\nInteraction (multiplication for numeric values, or binarized categorical values)\n\n.\nAll columns except the target/dependent variable\n\nIn order to specify transformations with this syntax, we need to import the relevant class. Then we go through the process of defining our formula. In this case we want to use all available variables (the .) and also add in the interactions between value1 and color and value2 and color, treating those as new features:"}, {"cell_type": "code", "execution_count": 3, "metadata": {}, "outputs": [], "source": "from pyspark.ml.feature import RFormula\n\nsupervised = RFormula(formula=\"lab ~ . + color:value1 + color:value2\")"}, {"cell_type": "markdown", "metadata": {}, "source": "At this point, we have declaratively specified how we would like to change our data into what we will train our model on. The next step is to fit the RFormula transformer to the data to let it discover the possible values of each column. Not all transformers have this requirement but because RFormula will automatically handle categorical variables for us, it needs to determine which columns are categorical and which are not, as well as what the distinct values of the categorical columns are. For this reason, we have to call the fit method. Once we call fit, it returns a \u201ctrained\u201d version of our transformer we can then use to actually transform our data.\n\nNow that we covered those details, let\u2019s continue on and prepare our DataFrame:"}, {"cell_type": "code", "execution_count": 4, "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "+-----+----+------+------------------+----------------------------------------------------------------------+-----+\n|color|lab |value1|value2            |features                                                              |label|\n+-----+----+------+------------------+----------------------------------------------------------------------+-----+\n|green|good|1     |14.386294994851129|(10,[1,2,3,5,8],[1.0,1.0,14.386294994851129,1.0,14.386294994851129])  |1.0  |\n|blue |bad |8     |14.386294994851129|(10,[2,3,6,9],[8.0,14.386294994851129,8.0,14.386294994851129])        |0.0  |\n|blue |bad |12    |14.386294994851129|(10,[2,3,6,9],[12.0,14.386294994851129,12.0,14.386294994851129])      |0.0  |\n|green|good|15    |38.97187133755819 |(10,[1,2,3,5,8],[1.0,15.0,38.97187133755819,15.0,38.97187133755819])  |1.0  |\n|green|good|12    |14.386294994851129|(10,[1,2,3,5,8],[1.0,12.0,14.386294994851129,12.0,14.386294994851129])|1.0  |\n+-----+----+------+------------------+----------------------------------------------------------------------+-----+\nonly showing top 5 rows\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "fittedRF = supervised.fit(df)  # fit the transformer\npreparedDF = fittedRF.transform(df)  # transform\npreparedDF.show(5, False)"}, {"cell_type": "markdown", "metadata": {}, "source": "In the output we can see the result of our transformation\u2014a column called features that has our previously raw data. What\u2019s happening behind the scenes is actually pretty simple. RFormula inspects our data during the fit call and outputs an object that will transform our data according to the specified formula, which is called an RFormulaModel. This \u201ctrained\u201d transformer always has the word Model in the type signature. When we use this transformer, Spark automatically converts our categorical variable to Doubles so that we can input it into a (yet to be specified) machine learning model. In particular, it assigns a numerical value to each possible color category, creates additional features for the interaction variables between colors and value1/value2, and puts them all into a single vector. We then call transform on that object in order to transform our input data into the expected output data.\n\nThus far you (pre)processed the data and added some features along the way. Now it is time to actually train a model (or a set of models) on this dataset. In order to do this, you first need to prepare a test set for evaluation.\n\nTIP: Having a good test set is probably the most important thing you can do to ensure you train a model you can actually use in the real world (in a dependable way). Not creating a representative test set or using your test set for hyperparameter tuning are surefire ways to create a model that does not perform well in real-world scenarios. Don\u2019t skip creating a test set\u2014it\u2019s a requirement to know how well your model actually does!\n\nLet\u2019s create a simple test set based off a random split of the data now (we\u2019ll be using this test set throughout the remainder of the notebook):"}, {"cell_type": "code", "execution_count": 5, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+-----+---+------+------------------+--------------------+-----+\n|color|lab|value1|            value2|            features|label|\n+-----+---+------+------------------+--------------------+-----+\n| blue|bad|     8|14.386294994851129|(10,[2,3,6,9],[8....|  0.0|\n| blue|bad|     8|14.386294994851129|(10,[2,3,6,9],[8....|  0.0|\n+-----+---+------+------------------+--------------------+-----+\nonly showing top 2 rows\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "train, test = preparedDF.randomSplit([0.7, 0.3], seed = 843)\ntest.show(2)"}, {"cell_type": "markdown", "metadata": {}, "source": "**Estimators**\n\nNow that we have transformed our data into the correct format and created some valuable features, it\u2019s time to actually fit our model. In this case we will use a classification algorithm called logistic regression. To create our classifier we instantiate an instance of *LogisticRegression*, using the default configuration or hyperparameters. We then set the label columns and the feature columns; the column names we are setting\u2014label and features\u2014are actually the default labels for all estimators in Spark MLlib, and in later notebooks we omit them:"}, {"cell_type": "code", "execution_count": 6, "metadata": {}, "outputs": [], "source": "from pyspark.ml.classification import LogisticRegression\n\nlr = LogisticRegression(labelCol=\"label\",featuresCol=\"features\")"}, {"cell_type": "markdown", "metadata": {}, "source": "Before we actually go about training this model, let\u2019s inspect the parameters. This is also a great way to remind yourself of the options available for each particular model:"}, {"cell_type": "code", "execution_count": 7, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "aggregationDepth: suggested depth for treeAggregate (>= 2). (default: 2)\nelasticNetParam: the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty. (default: 0.0)\nfamily: The name of family which is a description of the label distribution to be used in the model. Supported options: auto, binomial, multinomial (default: auto)\nfeaturesCol: features column name. (default: features, current: features)\nfitIntercept: whether to fit an intercept term. (default: True)\nlabelCol: label column name. (default: label, current: label)\nlowerBoundsOnCoefficients: The lower bounds on coefficients if fitting under bound constrained optimization. The bound matrix must be compatible with the shape (1, number of features) for binomial regression, or (number of classes, number of features) for multinomial regression. (undefined)\nlowerBoundsOnIntercepts: The lower bounds on intercepts if fitting under bound constrained optimization. The bounds vector size must beequal with 1 for binomial regression, or the number oflasses for multinomial regression. (undefined)\nmaxBlockSizeInMB: maximum memory in MB for stacking input data into blocks. Data is stacked within partitions. If more than remaining data size in a partition then it is adjusted to the data size. Default 0.0 represents choosing optimal value, depends on specific algorithm. Must be >= 0. (default: 0.0)\nmaxIter: max number of iterations (>= 0). (default: 100)\npredictionCol: prediction column name. (default: prediction)\nprobabilityCol: Column name for predicted class conditional probabilities. Note: Not all models output well-calibrated probability estimates! These probabilities should be treated as confidences, not precise probabilities. (default: probability)\nrawPredictionCol: raw prediction (a.k.a. confidence) column name. (default: rawPrediction)\nregParam: regularization parameter (>= 0). (default: 0.0)\nstandardization: whether to standardize the training features before fitting the model. (default: True)\nthreshold: Threshold in binary classification prediction, in range [0, 1]. If threshold and thresholds are both set, they must match.e.g. if threshold is p, then thresholds must be equal to [1-p, p]. (default: 0.5)\nthresholds: Thresholds in multi-class classification to adjust the probability of predicting each class. Array must have length equal to the number of classes, with values > 0, excepting that at most one value may be 0. The class with largest value p/t is predicted, where p is the original probability of that class and t is the class's threshold. (undefined)\ntol: the convergence tolerance for iterative algorithms (>= 0). (default: 1e-06)\nupperBoundsOnCoefficients: The upper bounds on coefficients if fitting under bound constrained optimization. The bound matrix must be compatible with the shape (1, number of features) for binomial regression, or (number of classes, number of features) for multinomial regression. (undefined)\nupperBoundsOnIntercepts: The upper bounds on intercepts if fitting under bound constrained optimization. The bound vector size must be equal with 1 for binomial regression, or the number of classes for multinomial regression. (undefined)\nweightCol: weight column name. If this is not set or empty, we treat all instance weights as 1.0. (undefined)\n"}], "source": "print(lr.explainParams())"}, {"cell_type": "markdown", "metadata": {}, "source": "The `explainParams` method exists on all algorithms available in MLlib.\n\nUpon instantiating an untrained algorithm, it becomes time to fit it to data. In this case, this returns a *LogisticRegressionModel*:"}, {"cell_type": "code", "execution_count": 8, "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "fittedLR = lr.fit(train)"}, {"cell_type": "markdown", "metadata": {}, "source": "This code will kick off a Spark job to train the model. As opposed to the transformations that you saw, the fitting of a machine learning model is eager and performed immediately.\n\nOnce complete, you can use the model to make predictions. Logically this means tranforming features into labels. We make predictions with the transform method. For example, **we can transform our training dataset to see what labels our model assigned to the training data and how those compare to the true outputs**. This, again, is just another DataFrame we can manipulate. Let\u2019s perform that prediction with the following code snippet:"}, {"cell_type": "code", "execution_count": 9, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+-----+----------+\n|label|prediction|\n+-----+----------+\n|  0.0|       0.0|\n|  0.0|       0.0|\n|  0.0|       0.0|\n|  0.0|       0.0|\n|  0.0|       0.0|\n+-----+----------+\nonly showing top 5 rows\n\n"}], "source": "fittedLR.transform(test).select(\"label\", \"prediction\").show(5)"}, {"cell_type": "markdown", "metadata": {}, "source": "### SUMMARY\nAll the above steps can be summarized in the following cell:"}, {"cell_type": "code", "execution_count": 10, "metadata": {}, "outputs": [], "source": "from pyspark.ml.feature import RFormula\nfrom pyspark.ml.classification import LogisticRegression\n\nsupervised = RFormula(formula=\"lab ~ . + color:value1 + color:value2\")  # define the R formula\nfittedRF = supervised.fit(df)  # fit the transformer\npreparedDF = fittedRF.transform(df)  # transform\n\ntrain, test = preparedDF.randomSplit([0.7, 0.3], seed = 843)  # split into train/test\nlr = LogisticRegression()  # instantiate an instance of LogisticRegression\nfittedLR = lr.fit(train)  # fit the estimator\n#fittedLR.transform(train).select(\"probability\", \"prediction\").show(5)  # checkout the prediction on train dataset"}, {"cell_type": "code", "execution_count": 11, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+-----------+----------+\n|probability|prediction|\n+-----------+----------+\n|[1.0,0.0]  |0.0       |\n|[1.0,0.0]  |0.0       |\n|[1.0,0.0]  |0.0       |\n|[1.0,0.0]  |0.0       |\n|[1.0,0.0]  |0.0       |\n+-----------+----------+\nonly showing top 5 rows\n\n"}], "source": "fittedLR.transform(test).select(\"probability\", \"prediction\").show(5, False)"}, {"cell_type": "markdown", "metadata": {}, "source": "Our next step would be to manually evaluate this model and calculate performance metrics like the true positive rate, false negative rate, and so on. We might then turn around and try a different set of parameters to see if those perform better. However, while this is a useful process, it can also be quite tedious. Spark helps you avoid manually trying different models and evaluation criteria by allowing you to specify your workload as a declarative pipeline of work that includes all your transformations as well as tuning your hyperparameters.\n\n**A REVIEW OF HYPERPARAMETERS**\n\nAlthough we mentioned them previously, let\u2019s more formally define hyperparameters. Hyperparameters are configuration parameters that affect the training process, such as model architecture and regularization. They are set prior to starting training. For instance, logistic regression has a hyperparameter that determines how much regularization should be performed on our data through the training phase (regularization is a technique that pushes models against overfitting data). Coming next you\u2019ll see that we can set up our pipeline to try different hyperparameter values (e.g., different regularization values) in order to compare different variations of the same model against one another.\n\n### Pipelining Our Workflow\n\nAs you probably noticed, if you are performing a lot of transformations, writing all the steps and keeping track of DataFrames ends up being quite tedious. That\u2019s why Spark includes the Pipeline concept. A pipeline allows you to set up a dataflow of the relevant transformations that ends with an estimator that is automatically tuned according to your specifications, resulting in a tuned model ready for use. Figure below illustrates this process:\n\n<img src=\"https://github.com/soltaniehha/Big-Data-Analytics-for-Business/blob/master/figs/09-02-Pipelining-the-ML-workflow.png?raw=true\" width=\"800\" align=\"center\"/>\n"}, {"cell_type": "markdown", "metadata": {}, "source": "Note that it is essential that instances of transformers or models are not reused across different pipelines. Always create a new instance of a model before creating another pipeline.\n\n**In order to make sure we don\u2019t overfit, we are going to create a holdout test set and tune our hyperparameters based on a validation set** (note that we create this validation set based on the original dataset, not the preparedDF used in the previous example):"}, {"cell_type": "code", "execution_count": 12, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+-----+---+------+------------------+\n|color|lab|value1|            value2|\n+-----+---+------+------------------+\n| blue|bad|     8|14.386294994851129|\n| blue|bad|     8|14.386294994851129|\n+-----+---+------+------------------+\nonly showing top 2 rows\n\n"}], "source": "train, test = df.randomSplit([0.7, 0.3], seed = 843)\ntest.show(2)"}, {"cell_type": "markdown", "metadata": {}, "source": "Now that you have a holdout set, let\u2019s create the base stages in our pipeline. A stage simply represents a transformer or an estimator. In our case, we will have two estimators. The RFomula will first analyze our data to understand the types of input features and then transform them to create new features. Subsequently, the LogisticRegression object is the algorithm that we will train to produce a model:"}, {"cell_type": "code", "execution_count": 13, "metadata": {}, "outputs": [], "source": "rForm = RFormula()\nlr = LogisticRegression().setLabelCol(\"label\").setFeaturesCol(\"features\")"}, {"cell_type": "markdown", "metadata": {}, "source": "We will set the potential values for the RFormula in the next section. Now instead of manually using our transformations and then tuning our model we just make them stages in the overall pipeline, as in the following code snippet:"}, {"cell_type": "code", "execution_count": 14, "metadata": {}, "outputs": [], "source": "from pyspark.ml import Pipeline\n\nstages = [rForm, lr]\npipeline = Pipeline().setStages(stages)"}, {"cell_type": "markdown", "metadata": {}, "source": "### Training and Evaluation\n\nNow that you arranged the logical pipeline, the next step is training. In our case, we won\u2019t train just one model (like we did previously); we will train several variations of the model by specifying different combinations of hyperparameters that we would like Spark to test. We will then select the best model using an Evaluator that compares their predictions on our validation data. We can test different hyperparameters in the entire pipeline, even in the RFormula that we use to manipulate the raw data. This code shows how we go about doing that:"}, {"cell_type": "code", "execution_count": 15, "metadata": {}, "outputs": [], "source": "from pyspark.ml.tuning import ParamGridBuilder\n\nparams = ParamGridBuilder()\\\n  .addGrid(rForm.formula, [\n    \"lab ~ . + color:value1\",\n    \"lab ~ . + color:value1 + color:value2\"])\\\n  .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0])\\\n  .addGrid(lr.regParam, [0.1, 2.0])\\\n  .build()"}, {"cell_type": "markdown", "metadata": {}, "source": "In our current paramter grid, there are three hyperparameters that will diverge from the defaults:\n\n* Two different versions of the RFormula\n\n* Three different options for the ElasticNet parameter\n\n* Two different options for the regularization parameter\n\nThis gives us a total of 12 different combinations of these parameters, which means we will be training 12 different versions of logistic regression. We explain the ElasticNet parameter as well as the regularization options in the next class.\n\nNow that the grid is built, it\u2019s time to specify our evaluation process. The evaluator allows us to automatically and objectively compare multiple models to the same evaluation metric. There are evaluators for classification and regression, covered in later notebooks, but in this case we will use the `BinaryClassificationEvaluator`, which has a number of potential evaluation metrics, as we\u2019ll discuss in the future notebooks. In this case we will use `areaUnderROC`, which is the total area under the receiver operating characteristic, a common measure of classification performance:"}, {"cell_type": "code", "execution_count": 16, "metadata": {}, "outputs": [], "source": "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n\nevaluator = BinaryClassificationEvaluator(metricName='areaUnderROC')"}, {"cell_type": "markdown", "metadata": {}, "source": "Now that we have a pipeline that specifies how our data should be transformed, we will perform model selection to try out different hyperparameters in our logistic regression model and measure success by comparing their performance using the areaUnderROC metric.\n\nAs we discussed, it is a best practice in machine learning to fit hyperparameters on a validation set (instead of your test set) to prevent overfitting. For this reason, we cannot use our holdout test set (that we created before) to tune these parameters. Luckily, Spark provides two options for performing hyperparameter tuning automatically. We can use **TrainValidationSplit**, which will simply perform an arbitrary random split of our data into two different groups, or **CrossValidator**, which performs K-fold cross-validation by splitting the dataset into k non-overlapping, randomly partitioned folds:"}, {"cell_type": "code", "execution_count": 17, "metadata": {}, "outputs": [], "source": "from pyspark.ml.tuning import TrainValidationSplit\n\ntvs = TrainValidationSplit()\\\n  .setTrainRatio(0.75)\\\n  .setEstimatorParamMaps(params)\\\n  .setEstimator(pipeline)\\\n  .setEvaluator(evaluator)"}, {"cell_type": "markdown", "metadata": {}, "source": "Let\u2019s run the entire pipeline we constructed. To review, running this pipeline will test out every version of the model against the validation set. Note the type of tvsFitted is TrainValidationSplitModel. Any time we fit a given model, it outputs a \u201cmodel\u201d type:"}, {"cell_type": "code", "execution_count": 18, "metadata": {}, "outputs": [], "source": "tvsFitted = tvs.fit(train)"}, {"cell_type": "markdown", "metadata": {}, "source": "And of course evaluate how it performs on the test set!"}, {"cell_type": "code", "execution_count": 19, "metadata": {}, "outputs": [{"data": {"text/plain": "1.0"}, "execution_count": 19, "metadata": {}, "output_type": "execute_result"}], "source": "evaluator.evaluate(tvsFitted.transform(test))"}, {"cell_type": "code", "execution_count": 20, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "aggregationDepth 2\nelasticNetParam 0.0\nfamily auto\nfeaturesCol features\nfitIntercept True\nlabelCol label\nmaxBlockSizeInMB 0.0\nmaxIter 100\npredictionCol prediction\nprobabilityCol probability\nrawPredictionCol rawPrediction\nregParam 0.1\nstandardization True\nthreshold 0.5\ntol 1e-06\n"}], "source": "bestParams = tvsFitted.bestModel.stages[-1].extractParamMap()\nfor param in bestParams:\n    print(param.name, bestParams[param])"}, {"cell_type": "code", "execution_count": 34, "metadata": {}, "outputs": [{"data": {"text/plain": "RFormulaModel: uid=RFormula_99142c29dcc3, resolvedFormula=ResolvedRFormula(label=lab, terms=[color,value1,value2,{color,value1},{color,value2}], hasIntercept=true)"}, "execution_count": 34, "metadata": {}, "output_type": "execute_result"}], "source": "tvsFitted.bestModel.stages[0]"}, {"cell_type": "markdown", "metadata": {}, "source": "### SUMMARY\nAll the above steps can be summarized in the following cell:"}, {"cell_type": "code", "execution_count": 21, "metadata": {}, "outputs": [{"data": {"text/plain": "1.0"}, "execution_count": 21, "metadata": {}, "output_type": "execute_result"}], "source": "from pyspark.ml import Pipeline\nfrom pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator\n\ntrain, test = df.randomSplit([0.7, 0.3], seed = 843)  # create a holdout set before transformation\nrForm = RFormula()  # defining stage 1 by creating an empty R formula\nlr = LogisticRegression().setLabelCol(\"label\").setFeaturesCol(\"features\")  # defining stage 2 by instantiating an instance of LogisticRegression\n\nstages = [rForm, lr]  # setting the stages\npipeline = Pipeline().setStages(stages)  # adding the stages to the pipeline\n\n# building the hyperparameter grid\nparams = ParamGridBuilder()\\\n  .addGrid(rForm.formula, [\n    \"lab ~ . + color:value1\",\n    \"lab ~ . + color:value1 + color:value2\"])\\\n  .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0])\\\n  .addGrid(lr.regParam, [0.1, 2.0])\\\n  .build()\n\n# setting the evaluator as AUC\nevaluator = BinaryClassificationEvaluator(metricName='areaUnderROC')\n\n# defining Train Validation Split to be used for hypyerparameter tuning\ntvs = TrainValidationSplit()\\\n  .setTrainRatio(0.75)\\\n  .setEstimatorParamMaps(params)\\\n  .setEstimator(pipeline)\\\n  .setEvaluator(evaluator)\n\ntvsFitted = tvs.fit(train)  # fit the estimator\n\nevaluator.evaluate(tvsFitted.transform(test))  # evaluate the test set (AUC)"}, {"cell_type": "markdown", "metadata": {}, "source": "### Persisting and Applying Models\n\nNow that we trained this model, we can persist it to disk to use it for prediction purposes later on:"}, {"cell_type": "code", "execution_count": 22, "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "tvsFitted.bestModel.write().overwrite().save(data + \"model/firstModel\")"}, {"cell_type": "markdown", "metadata": {}, "source": "After writing out the model, we can load it into another Spark program to make predictions:"}, {"cell_type": "code", "execution_count": 23, "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "from pyspark.ml import PipelineModel\n\nmodel = PipelineModel.load(data + \"model/firstModel\")\nprediction = model.transform(test)"}, {"cell_type": "code", "execution_count": 24, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+----------------------------------------+----------+-----+\n|probability                             |prediction|label|\n+----------------------------------------+----------+-----+\n|[0.9233050601958284,0.07669493980417164]|0.0       |0.0  |\n|[0.9233050601958284,0.07669493980417164]|0.0       |0.0  |\n|[0.9233050601958284,0.07669493980417164]|0.0       |0.0  |\n|[0.9348943057269108,0.06510569427308921]|0.0       |0.0  |\n|[0.9348943057269108,0.06510569427308921]|0.0       |0.0  |\n|[0.49777956051405065,0.5022204394859493]|1.0       |0.0  |\n|[0.4003383700385894,0.5996616299614106] |1.0       |1.0  |\n|[0.4003383700385894,0.5996616299614106] |1.0       |1.0  |\n|[0.4003383700385894,0.5996616299614106] |1.0       |1.0  |\n|[0.4714656760682543,0.5285343239317457] |1.0       |1.0  |\n+----------------------------------------+----------+-----+\nonly showing top 10 rows\n\n"}], "source": "prediction.select(\"probability\", \"prediction\", \"label\").show(10, False)"}, {"cell_type": "markdown", "metadata": {}, "source": "### Deployment Patterns\n\nIn Spark there are several different deployment patterns for putting machine learning models into production. Figure below illustrates common workflows.\n\n<img src=\"https://github.com/soltaniehha/Big-Data-Analytics-for-Business/blob/master/figs/09-02-productionization-process.png?raw=true\" width=\"800\" align=\"center\"/>\n"}, {"cell_type": "markdown", "metadata": {}, "source": "Here are the various options for how you might go about deploying a Spark model. These are the general options you should be able to link to the process illustrated in the figure above.\n\n* Train your machine learning (ML) model offline and then supply it with offline data. In this context, we mean offline data to be data that is stored for analysis, and not data that you need to get an answer from quickly. Spark is well suited to this sort of deployment.\n\n* Train your model offline and then put the results into a database (usually a key-value store). This works well for something like recommendation but poorly for something like classification or regression where you cannot just look up a value for a given user but must calculate one based on the input.\n\n* Train your ML algorithm offline, persist the model to disk, and then use that for serving. This is not a low-latency solution if you use Spark for the serving part, as the overhead of starting up a Spark job can be high, even if you\u2019re not running on a cluster. Additionally this does not parallelize well, so you\u2019ll likely have to put a load balancer in front of multiple model replicas and build out some REST API integration yourself. There are some interesting potential solutions to this problem, but no standards currently exist for this sort of model serving.\n\n* Manually (or via some other software) convert your distributed model to one that can run much more quickly on a single machine. This works well when there is not too much manipulation of the raw data in Spark but can be hard to maintain over time. Again, there are several solutions in progress. For example, MLlib can export some models to PMML, a common model interchange format.\n\n* Train your ML algorithm online and use it online. This is possible when used in conjunction with Structured Streaming, but can be complex for some models.\n\nWhile these are some of the options, there are many other ways of performing model deployment and management. This is an area under heavy development and many potential innovations are currently being worked on."}], "metadata": {"kernelspec": {"display_name": "PySpark", "language": "python", "name": "pyspark"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.10.8"}}, "nbformat": 4, "nbformat_minor": 4}